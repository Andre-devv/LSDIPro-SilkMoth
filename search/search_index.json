{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83e\udd8b LSDIPro SS2025 \ud83d\udcc4 SilkMoth: An Efficient Method for Finding Related Sets A project inspired by the SilkMoth paper, exploring efficient techniques for related set discovery. \ud83d\udc65 Team Members Andreas Wilms Sarra Daknou Amina Iqbal Jakob Berschneider \ud83d\udcca See Experiments and Results \ud83d\udcd8 Project Documentation Table of Contents 1. Large Scale Data Integration Project (LSDIPro) 2. What is SilkMoth? \ud83d\udc1b 3. The Problem \ud83e\udde9 4. SilkMoth\u2019s Solution \ud83d\ude80 5. Core Pipeline Steps \ud83d\udd01 5.1 Tokenization 5.2 Inverted Index Construction 5.3 Signature Generation 5.4 Candidate Selection 5.5 Refinement Filters 5.6 Verification via Maximum Matching 6. Modes of Operation \ud83e\uddea 7. Supported Similarity Functions \ud83d\udcd0 8. Installing from Source 9. Experiment Results 1. Large Scale Data Integration Project (LSDIPro) As part of the university project LSDIPro, our team implemented the SilkMoth paper in Python. The course focuses on large-scale data integration, where student groups reproduce and extend research prototypes. The project emphasizes scalable algorithm design, evaluation, and handling heterogeneous data at scale. 2. What is SilkMoth? SilkMoth is a system designed to efficiently discover related sets in large collections of data, even when the elements within those sets are only approximately similar. This is especially important in data integration , data cleaning , and information retrieval , where messy or inconsistent data is common. 3. The Problem Determining whether two sets are related, for example, whether two database columns should be joined, often involves comparing their elements using similarity functions (not just exact matches). A powerful approach models this as a bipartite graph and finds the maximum matching score between elements. However, this method is computationally expensive ( O(n\u00b3) per pair), making it impractical for large datasets. 4. SilkMoth\u2019s Solution SilkMoth tackles this with a three-step approach: Signature Generation : Creates compact signatures for each set, ensuring related sets share signature parts. Pruning : Filters out unrelated sets early, reducing candidates. Verification : Applies the costly matching metric only on remaining candidates, matching brute-force accuracy but faster. 5. Core Pipeline Steps Figure 1. SILKMOTH pipeline framework. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0. 5.1 Tokenization Each element in every set is tokenized based on the selected similarity function: - Jaccard Similarity : Elements are split into whitespace-delimited tokens. - Edit Similarity : Elements are split into overlapping q -grams (e.g., 3-grams). 5.2 Inverted Index Construction An inverted index is built from the reference set R to map each token to a list of (set, element) pairs in which it occurs. This allows fast lookup of candidate sets sharing tokens with a query. 5.3 Signature Generation A signature is a subset of tokens selected from each set such that: - Any related set must share at least one signature token. - Signature size is minimized to reduce candidate space. Signature selection heuristics (e.g., cost/value greedy ranking) approximate the optimal valid signature, which is NP-complete to compute exactly. 5.4 Candidate Selection For each set R , retrieve from the inverted index all sets S sharing at least one token with R \u2019s signature. These become candidate sets for further evaluation. 5.5 Refinement Filters Two filters reduce false positives among candidates: - Check Filter : Uses an upper bound on similarity to eliminate sets below threshold. - Nearest Neighbor Filter : Approximates maximum matching score using nearest neighbor similarity for each element in R . 5.6 Verification via Maximum Matching Compute maximum weighted bipartite matching between elements of R and S for remaining candidates using the similarity function as edge weights. Sets meeting or exceeding threshold \u03b4 are considered related . 6. Modes of Operation \ud83e\uddea Discovery Mode : Compare all pairs of sets to find all related pairs. Use case: Finding related columns in databases. Search Mode : Given a reference set, find all related sets. Use case: Schema matching or entity deduplication. 7. Supported Similarity Functions \ud83d\udcd0 Jaccard Similarity Edit Similarity (Levenshtein-based) Optional minimum similarity threshold \u03b1 on element comparisons. 8. Installing from Source Run pip install src/ to install (Optional) Run python -m unittest discover -s src/silkmoth/test -p \"*.py\" to execute unit tests 9. Experiment Results \ud83d\udcca See Experiments and Results","title":"Home"},{"location":"#lsdipro-ss2025","text":"","title":"\ud83e\udd8b LSDIPro SS2025"},{"location":"#silkmoth-an-efficient-method-for-finding-related-sets","text":"A project inspired by the SilkMoth paper, exploring efficient techniques for related set discovery.","title":"\ud83d\udcc4 SilkMoth: An Efficient Method for Finding Related Sets"},{"location":"#team-members","text":"Andreas Wilms Sarra Daknou Amina Iqbal Jakob Berschneider","title":"\ud83d\udc65 Team Members"},{"location":"#see-experiments-and-results","text":"","title":"\ud83d\udcca See Experiments and Results"},{"location":"#project-documentation","text":"","title":"\ud83d\udcd8 Project Documentation"},{"location":"#table-of-contents","text":"1. Large Scale Data Integration Project (LSDIPro) 2. What is SilkMoth? \ud83d\udc1b 3. The Problem \ud83e\udde9 4. SilkMoth\u2019s Solution \ud83d\ude80 5. Core Pipeline Steps \ud83d\udd01 5.1 Tokenization 5.2 Inverted Index Construction 5.3 Signature Generation 5.4 Candidate Selection 5.5 Refinement Filters 5.6 Verification via Maximum Matching 6. Modes of Operation \ud83e\uddea 7. Supported Similarity Functions \ud83d\udcd0 8. Installing from Source 9. Experiment Results","title":"Table of Contents"},{"location":"#1-large-scale-data-integration-project-lsdipro","text":"As part of the university project LSDIPro, our team implemented the SilkMoth paper in Python. The course focuses on large-scale data integration, where student groups reproduce and extend research prototypes. The project emphasizes scalable algorithm design, evaluation, and handling heterogeneous data at scale.","title":"1. Large Scale Data Integration Project (LSDIPro)"},{"location":"#2-what-is-silkmoth","text":"SilkMoth is a system designed to efficiently discover related sets in large collections of data, even when the elements within those sets are only approximately similar. This is especially important in data integration , data cleaning , and information retrieval , where messy or inconsistent data is common.","title":"2. What is SilkMoth?"},{"location":"#3-the-problem","text":"Determining whether two sets are related, for example, whether two database columns should be joined, often involves comparing their elements using similarity functions (not just exact matches). A powerful approach models this as a bipartite graph and finds the maximum matching score between elements. However, this method is computationally expensive ( O(n\u00b3) per pair), making it impractical for large datasets.","title":"3. The Problem"},{"location":"#4-silkmoths-solution","text":"SilkMoth tackles this with a three-step approach: Signature Generation : Creates compact signatures for each set, ensuring related sets share signature parts. Pruning : Filters out unrelated sets early, reducing candidates. Verification : Applies the costly matching metric only on remaining candidates, matching brute-force accuracy but faster.","title":"4. SilkMoth\u2019s Solution"},{"location":"#5-core-pipeline-steps","text":"Figure 1. SILKMOTH pipeline framework. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0.","title":"5. Core Pipeline Steps"},{"location":"#51-tokenization","text":"Each element in every set is tokenized based on the selected similarity function: - Jaccard Similarity : Elements are split into whitespace-delimited tokens. - Edit Similarity : Elements are split into overlapping q -grams (e.g., 3-grams).","title":"5.1 Tokenization"},{"location":"#52-inverted-index-construction","text":"An inverted index is built from the reference set R to map each token to a list of (set, element) pairs in which it occurs. This allows fast lookup of candidate sets sharing tokens with a query.","title":"5.2 Inverted Index Construction"},{"location":"#53-signature-generation","text":"A signature is a subset of tokens selected from each set such that: - Any related set must share at least one signature token. - Signature size is minimized to reduce candidate space. Signature selection heuristics (e.g., cost/value greedy ranking) approximate the optimal valid signature, which is NP-complete to compute exactly.","title":"5.3 Signature Generation"},{"location":"#54-candidate-selection","text":"For each set R , retrieve from the inverted index all sets S sharing at least one token with R \u2019s signature. These become candidate sets for further evaluation.","title":"5.4 Candidate Selection"},{"location":"#55-refinement-filters","text":"Two filters reduce false positives among candidates: - Check Filter : Uses an upper bound on similarity to eliminate sets below threshold. - Nearest Neighbor Filter : Approximates maximum matching score using nearest neighbor similarity for each element in R .","title":"5.5 Refinement Filters"},{"location":"#56-verification-via-maximum-matching","text":"Compute maximum weighted bipartite matching between elements of R and S for remaining candidates using the similarity function as edge weights. Sets meeting or exceeding threshold \u03b4 are considered related .","title":"5.6 Verification via Maximum Matching"},{"location":"#6-modes-of-operation","text":"Discovery Mode : Compare all pairs of sets to find all related pairs. Use case: Finding related columns in databases. Search Mode : Given a reference set, find all related sets. Use case: Schema matching or entity deduplication.","title":"6. Modes of Operation \ud83e\uddea"},{"location":"#7-supported-similarity-functions","text":"Jaccard Similarity Edit Similarity (Levenshtein-based) Optional minimum similarity threshold \u03b1 on element comparisons.","title":"7. Supported Similarity Functions \ud83d\udcd0"},{"location":"#8-installing-from-source","text":"Run pip install src/ to install (Optional) Run python -m unittest discover -s src/silkmoth/test -p \"*.py\" to execute unit tests","title":"8. Installing from Source"},{"location":"#9-experiment-results","text":"\ud83d\udcca See Experiments and Results","title":"9. Experiment Results"},{"location":"experiments/","text":"\ud83e\uddea Running the Experiments This project includes multiple experiments to evaluate the performance and accuracy of our Python implementation of SilkMoth . \ud83d\udcca 1. Experiment Types You can replicate and customize the following types of experiments using different configurations (e.g., filters, signature strategies, reduction techniques): String Matching (DBLP Publication Titles) Schema Matching (WebTables) Inclusion Dependency Discovery (WebTable Columns) Exact descriptions can be found in the official paper. \ud83d\udce6 2. WebSchema Inclusion Dependency Setup To run the WebSchema + Inclusion Dependency experiments: Download the pre-extracted dataset from \ud83d\udce5 this link . Place the .json files in the data/webtables/ directory (create the folder if it does not exist) . \ud83d\ude80 3. Running the Experiments To execute the core experiments from the paper: python run.py \ud83d\udcc8 4. Results Overview We compared our results with those presented in the original SilkMoth paper. Although exact reproduction is not possible due to language differences (Python vs C++) and dataset variations, overall performance trends align well . All the results can be found in the folder results . \ud83d\udca1 Recent performance enhancements leverage scipy \u2019s C-accelerated matching, replacing the original networkx -based approach. Unless otherwise specified, the diagrams shown are generated using the networkx implementation. \ud83d\udd0d Inclusion Dependency Goal : Check if each reference set is contained within source sets. Filter Comparison Signature Comparison Reduction Comparison Scalability \ud83d\udd0d Schema Matching (WebTables) Goal : Detect related set pairs within a single source set. Filter Comparison Signature Comparison Scalability \ud83d\udd0d Additional: Inclusion Dependency SilkMoth Filter compared with no SilkMoth In this analysis, we focus exclusively on SilkMoth. But how does it compare to a brute-force approach that skips the SilkMoth pipeline entirely? The graph below shows the Filter run alongside the brute-force bipartite matching method without any optimization pipeline. The results clearly demonstrate a dramatic improvement in runtime efficiency when using SilkMoth. \ud83d\udd0d Additional: Schema Matching with GitHub WebTables Similar to Schema Matching, this experiment uses a GitHub WebTable as a fixed reference set and matches it against other sets. The goal is to evaluate SilkMoth\u2019s performance across different domains. Left: Matching with one reference set. Right: Matching with WebTable Corpus and GitHub WebTable datasets. The results show no significant difference, indicating consistent behavior across varying datasets.","title":"Index"},{"location":"experiments/#running-the-experiments","text":"This project includes multiple experiments to evaluate the performance and accuracy of our Python implementation of SilkMoth .","title":"\ud83e\uddea Running the Experiments"},{"location":"experiments/#1-experiment-types","text":"You can replicate and customize the following types of experiments using different configurations (e.g., filters, signature strategies, reduction techniques): String Matching (DBLP Publication Titles) Schema Matching (WebTables) Inclusion Dependency Discovery (WebTable Columns) Exact descriptions can be found in the official paper.","title":"\ud83d\udcca 1. Experiment Types"},{"location":"experiments/#2-webschema-inclusion-dependency-setup","text":"To run the WebSchema + Inclusion Dependency experiments: Download the pre-extracted dataset from \ud83d\udce5 this link . Place the .json files in the data/webtables/ directory (create the folder if it does not exist) .","title":"\ud83d\udce6 2. WebSchema Inclusion Dependency Setup"},{"location":"experiments/#3-running-the-experiments","text":"To execute the core experiments from the paper: python run.py","title":"\ud83d\ude80 3. Running the Experiments"},{"location":"experiments/#4-results-overview","text":"We compared our results with those presented in the original SilkMoth paper. Although exact reproduction is not possible due to language differences (Python vs C++) and dataset variations, overall performance trends align well . All the results can be found in the folder results . \ud83d\udca1 Recent performance enhancements leverage scipy \u2019s C-accelerated matching, replacing the original networkx -based approach. Unless otherwise specified, the diagrams shown are generated using the networkx implementation.","title":"\ud83d\udcc8 4. Results Overview"},{"location":"experiments/#inclusion-dependency","text":"Goal : Check if each reference set is contained within source sets. Filter Comparison Signature Comparison Reduction Comparison Scalability","title":"\ud83d\udd0d Inclusion Dependency"},{"location":"experiments/#schema-matching-webtables","text":"Goal : Detect related set pairs within a single source set. Filter Comparison Signature Comparison Scalability","title":"\ud83d\udd0d Schema Matching (WebTables)"},{"location":"experiments/#additional-inclusion-dependency-silkmoth-filter-compared-with-no-silkmoth","text":"In this analysis, we focus exclusively on SilkMoth. But how does it compare to a brute-force approach that skips the SilkMoth pipeline entirely? The graph below shows the Filter run alongside the brute-force bipartite matching method without any optimization pipeline. The results clearly demonstrate a dramatic improvement in runtime efficiency when using SilkMoth.","title":"\ud83d\udd0d Additional: Inclusion Dependency SilkMoth Filter compared with no SilkMoth"},{"location":"experiments/#additional-schema-matching-with-github-webtables","text":"Similar to Schema Matching, this experiment uses a GitHub WebTable as a fixed reference set and matches it against other sets. The goal is to evaluate SilkMoth\u2019s performance across different domains. Left: Matching with one reference set. Right: Matching with WebTable Corpus and GitHub WebTable datasets. The results show no significant difference, indicating consistent behavior across varying datasets.","title":"\ud83d\udd0d Additional: Schema Matching with GitHub WebTables"},{"location":"pages/candidate_selector/","text":"CandidateSelector The candidate selector executes the candidate selection step in the SilkMoth pipeline. After signature generation SilkMoth accesses the inverted index to get all sets which contain a token in the signature to form an initial set of candidates. The size of the candidate set can be reduced further by applying the check or nearest neighbour filters in the refinement step of the SilkMoth pipeline. Examples >>> from silkmoth.candidate_selector import CandidateSelector >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Something\", \"Else\"}] >>> S3 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2, S3] >>> signature = [\"Apple\", \"Berlin\"] >>> I = InvertedIndex(S) >>> cand_selector = CandidateSelector(jaccard_similarity, similar, 0.7) >>> cand_selector.get_candidates(signature, I, 2) {0, 2} Source code in src/silkmoth/candidate_selector.py class CandidateSelector: \"\"\" The candidate selector executes the candidate selection step in the SilkMoth pipeline. After signature generation SilkMoth accesses the inverted index to get all sets which contain a token in the signature to form an initial set of candidates. The size of the candidate set can be reduced further by applying the check or nearest neighbour filters in the refinement step of the SilkMoth pipeline. Examples -------- ``` >>> from silkmoth.candidate_selector import CandidateSelector >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Something\", \"Else\"}] >>> S3 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2, S3] >>> signature = [\"Apple\", \"Berlin\"] >>> I = InvertedIndex(S) >>> cand_selector = CandidateSelector(jaccard_similarity, similar, 0.7) >>> cand_selector.get_candidates(signature, I, 2) {0, 2} ``` \"\"\" def __init__(self, similarity_func, sim_metric, related_thresh, sim_thresh=0.0, q = 3): \"\"\" Initialize the candidate selector with some parameters. Args: similarity_func (callable): Similarity function phi(r, s) (e.g., Jaccard). sim_metric (callable): Similarity metric related(R, S) (e.g., contain). related_thresh (float): Relatedness threshold delta. sim_thresh (float): Similarity threshold alpha. q (int): q-chunk length for edit similarity. \"\"\" self.similarity = similarity_func self.sim_metric = sim_metric self.delta = related_thresh self.alpha = sim_thresh self.q = q def get_candidates(self, signature, inverted_index, ref_size) -> set: \"\"\" Retrieve candidate set indices using token signature lookup. Args: signature (list): Signature tokens for a reference set. inverted_index (InvertedIndex): Instance of the custom InvertedIndex class. ref_size (int): Size of set R. Returns: set: Indices of candidate sets containing at least one signature token. \"\"\" candidates = set() for token in signature: try: idx_list = inverted_index.get_indexes(token) for set_idx, _ in idx_list: src_size = len(inverted_index.get_set(set_idx)) if self.verify_size(ref_size, src_size): candidates.add(set_idx) except ValueError: # token not found in inverted index; safely ignore continue return candidates def verify_size(self, ref_size, src_size) -> bool: \"\"\" Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Args: ref_size (int): Size of set R. src_size (int): Size of (possible) set S. Returns: bool: True if both sets could be related based on their size, False otherwise. \"\"\" # case 1: Set-Containment if self.sim_metric == contain and ref_size > src_size: return False # case 2: Set-Similarity if self.sim_metric == similar: if min(ref_size, src_size) < self.delta * max(ref_size, src_size): return False return True def check_filter(self, R, K, candidates, inverted_index) -> tuple: \"\"\" Apply check filter to prune weak candidate sets. Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate set indices from get_candidates(). inverted_index (InvertedIndex): For retrieving sets. Returns: tuple: set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. \"\"\" filtered = set() match_map = dict() k_i_sets = [set(r_i).intersection(K) for r_i in R] for c_idx in candidates: matched = self.create_match_map(R, k_i_sets, c_idx, inverted_index) if matched: filtered.add(c_idx) match_map[c_idx] = matched return filtered, match_map def create_match_map(self, R, k_i_sets, c_idx, inverted_index) -> dict: \"\"\" Create a match map for a specific candidate index. Args: R (list of list): Tokenized reference set. k_i_sets (list of sets): Unflattened signature. c_idx (int): Candidate set index. inverted_index (InvertedIndex): For retrieving sets. Returns: dict: r_idx -> max_sim for matched reference sets. \"\"\" S = inverted_index.get_set(c_idx) matched = {} for r_idx, (r_i, k_i) in enumerate(zip(R, k_i_sets)): if not r_i or not k_i: continue denominator = len(r_i) threshold = (denominator - len(k_i)) / denominator if denominator != 0 else 0.0 is_edit = self.similarity in (edit_similarity, N_edit_similarity) # for Jaccard set is needed, for edit list is needed if not is_edit: r_set = set(r_i) max_sim = 0.0 for token in k_i: try: entries = inverted_index.get_indexes_binary(token, c_idx) for s_idx, e_idx in entries: if s_idx != c_idx: continue s = S[e_idx] # call signature based on edit vs. jaccard if is_edit: sim = self.similarity(r_i, s, self.alpha) else: sim = self.similarity(r_set, set(s), self.alpha) if sim >= threshold: max_sim = max(max_sim, sim) except ValueError: continue if max_sim >= threshold: matched[r_idx] = max_sim return matched def _nn_search(self, r_elem, S, c_idx, inverted_index) -> float: \"\"\" Find the maximum similarity between r and elements s \u2208 S[C] that share at least one token with r using the inverted index for efficiency. Args: r_set (set): Reference element tokens. S (list of list): Elements of candidate set S[c_idx]. c_idx (int): Index of candidate set in inverted index. inverted_index (InvertedIndex): For fetching token locations. Returns: float: Maximum similarity between r and any s \u2208 S[c_idx]. \"\"\" # seen = set() max_sim = 0.0 is_edit = self.similarity in (edit_similarity, N_edit_similarity) for token in r_elem: try: entries = inverted_index.get_indexes_binary(token,c_idx) for s_idx, e_idx in entries: if s_idx != c_idx: continue s = S[e_idx] if is_edit: sim = self.similarity(r_elem, s, self.alpha) else: sim = self.similarity(set(r_elem), set(s), self.alpha) max_sim = max(max_sim, sim) except ValueError: continue return max_sim def nn_filter(self, R, K, candidates, inverted_index, threshold, match_map) -> set: \"\"\" Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate indices from check filter. inverted_index (InvertedIndex): To retrieve sets and indexes. threshold (float): Relatedness threshold \u03b4 (between 0 and 1). match_map (dict): Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). Returns: set: Final filtered candidate indices that pass the NN filter. \"\"\" n = len(R) theta = threshold * n is_edit = self.similarity in (edit_similarity, N_edit_similarity) k_i_sets = [set(r_i).intersection(K) for r_i in R] r_i_list = R final_filtered = set() total_init = 0 for r_idx, r_i in enumerate(R): if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) total_init += base_loss for c_idx in candidates: S = inverted_index.get_set(c_idx) if self.alpha > 0: S_tokens = set() for s in S: S_tokens.update(s) # Check if match_map is provided, otherwise create it if match_map is None: matched = self.create_match_map(R, K, c_idx, inverted_index) else: matched = match_map.get(c_idx, {}) # Step 1: initialize total estimate total = total_init # Step 2: for matched r\u1d62, computational reuse of sim and adjust total if matched: for r_idx, sim in matched.items(): r_i = r_i_list[r_idx] if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) total += sim - base_loss # Step 3: for non-matched r\u1d62, compute NN and adjust total for r_idx in set(range(n)) - matched.keys(): r_i = r_i_list[r_idx] if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) r_set = set(r_i) # Case alpha > 0 if (self.alpha > 0 and len(k_i) >= floor((1 - self.alpha) * len(r_i)) + 1 and k_i.isdisjoint(S_tokens)): nn_sim = 0 else: if is_edit: # brute\u2011force search over q\u2011gram lists for edit_similarity r_list = r_i_list[r_idx] nn_sim = 0.0 for s_list in S: sim = self.similarity(r_list, s_list, self.alpha) if sim > nn_sim: nn_sim = sim else: # inverted\u2010index search for jaccard nn_sim = self._nn_search(r_set, S, c_idx, inverted_index) total += nn_sim - base_loss if total < theta: break if total >= theta: final_filtered.add(c_idx) return final_filtered def calc_base_loss(self, k_i, r_i): if self.similarity in (edit_similarity, N_edit_similarity): denominator = len(r_i) + len(k_i) base_loss = len(r_i) / denominator if denominator != 0 else 0.0 else: denominator = len(r_i) base_loss = (len(r_i) - len(k_i)) / denominator if denominator != 0 else 0.0 return base_loss __init__(similarity_func, sim_metric, related_thresh, sim_thresh=0.0, q=3) Initialize the candidate selector with some parameters. Parameters: Name Type Description Default similarity_func callable Similarity function phi(r, s) (e.g., Jaccard). required sim_metric callable Similarity metric related(R, S) (e.g., contain). required related_thresh float Relatedness threshold delta. required sim_thresh float Similarity threshold alpha. 0.0 q int q-chunk length for edit similarity. 3 Source code in src/silkmoth/candidate_selector.py def __init__(self, similarity_func, sim_metric, related_thresh, sim_thresh=0.0, q = 3): \"\"\" Initialize the candidate selector with some parameters. Args: similarity_func (callable): Similarity function phi(r, s) (e.g., Jaccard). sim_metric (callable): Similarity metric related(R, S) (e.g., contain). related_thresh (float): Relatedness threshold delta. sim_thresh (float): Similarity threshold alpha. q (int): q-chunk length for edit similarity. \"\"\" self.similarity = similarity_func self.sim_metric = sim_metric self.delta = related_thresh self.alpha = sim_thresh self.q = q check_filter(R, K, candidates, inverted_index) Apply check filter to prune weak candidate sets. Parameters: Name Type Description Default R list of list Tokenized reference set. required K set Flattened signature tokens. required candidates set Candidate set indices from get_candidates(). required inverted_index InvertedIndex For retrieving sets. required Returns: Name Type Description tuple tuple set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. Source code in src/silkmoth/candidate_selector.py def check_filter(self, R, K, candidates, inverted_index) -> tuple: \"\"\" Apply check filter to prune weak candidate sets. Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate set indices from get_candidates(). inverted_index (InvertedIndex): For retrieving sets. Returns: tuple: set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. \"\"\" filtered = set() match_map = dict() k_i_sets = [set(r_i).intersection(K) for r_i in R] for c_idx in candidates: matched = self.create_match_map(R, k_i_sets, c_idx, inverted_index) if matched: filtered.add(c_idx) match_map[c_idx] = matched return filtered, match_map create_match_map(R, k_i_sets, c_idx, inverted_index) Create a match map for a specific candidate index. Parameters: Name Type Description Default R list of list Tokenized reference set. required k_i_sets list of sets Unflattened signature. required c_idx int Candidate set index. required inverted_index InvertedIndex For retrieving sets. required Returns: Name Type Description dict dict r_idx -> max_sim for matched reference sets. Source code in src/silkmoth/candidate_selector.py def create_match_map(self, R, k_i_sets, c_idx, inverted_index) -> dict: \"\"\" Create a match map for a specific candidate index. Args: R (list of list): Tokenized reference set. k_i_sets (list of sets): Unflattened signature. c_idx (int): Candidate set index. inverted_index (InvertedIndex): For retrieving sets. Returns: dict: r_idx -> max_sim for matched reference sets. \"\"\" S = inverted_index.get_set(c_idx) matched = {} for r_idx, (r_i, k_i) in enumerate(zip(R, k_i_sets)): if not r_i or not k_i: continue denominator = len(r_i) threshold = (denominator - len(k_i)) / denominator if denominator != 0 else 0.0 is_edit = self.similarity in (edit_similarity, N_edit_similarity) # for Jaccard set is needed, for edit list is needed if not is_edit: r_set = set(r_i) max_sim = 0.0 for token in k_i: try: entries = inverted_index.get_indexes_binary(token, c_idx) for s_idx, e_idx in entries: if s_idx != c_idx: continue s = S[e_idx] # call signature based on edit vs. jaccard if is_edit: sim = self.similarity(r_i, s, self.alpha) else: sim = self.similarity(r_set, set(s), self.alpha) if sim >= threshold: max_sim = max(max_sim, sim) except ValueError: continue if max_sim >= threshold: matched[r_idx] = max_sim return matched get_candidates(signature, inverted_index, ref_size) Retrieve candidate set indices using token signature lookup. Parameters: Name Type Description Default signature list Signature tokens for a reference set. required inverted_index InvertedIndex Instance of the custom InvertedIndex class. required ref_size int Size of set R. required Returns: Name Type Description set set Indices of candidate sets containing at least one signature token. Source code in src/silkmoth/candidate_selector.py def get_candidates(self, signature, inverted_index, ref_size) -> set: \"\"\" Retrieve candidate set indices using token signature lookup. Args: signature (list): Signature tokens for a reference set. inverted_index (InvertedIndex): Instance of the custom InvertedIndex class. ref_size (int): Size of set R. Returns: set: Indices of candidate sets containing at least one signature token. \"\"\" candidates = set() for token in signature: try: idx_list = inverted_index.get_indexes(token) for set_idx, _ in idx_list: src_size = len(inverted_index.get_set(set_idx)) if self.verify_size(ref_size, src_size): candidates.add(set_idx) except ValueError: # token not found in inverted index; safely ignore continue return candidates nn_filter(R, K, candidates, inverted_index, threshold, match_map) Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Parameters: Name Type Description Default R list of list Tokenized reference set. required K set Flattened signature tokens. required candidates set Candidate indices from check filter. required inverted_index InvertedIndex To retrieve sets and indexes. required threshold float Relatedness threshold \u03b4 (between 0 and 1). required match_map dict Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). required Returns: Name Type Description set set Final filtered candidate indices that pass the NN filter. Source code in src/silkmoth/candidate_selector.py def nn_filter(self, R, K, candidates, inverted_index, threshold, match_map) -> set: \"\"\" Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate indices from check filter. inverted_index (InvertedIndex): To retrieve sets and indexes. threshold (float): Relatedness threshold \u03b4 (between 0 and 1). match_map (dict): Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). Returns: set: Final filtered candidate indices that pass the NN filter. \"\"\" n = len(R) theta = threshold * n is_edit = self.similarity in (edit_similarity, N_edit_similarity) k_i_sets = [set(r_i).intersection(K) for r_i in R] r_i_list = R final_filtered = set() total_init = 0 for r_idx, r_i in enumerate(R): if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) total_init += base_loss for c_idx in candidates: S = inverted_index.get_set(c_idx) if self.alpha > 0: S_tokens = set() for s in S: S_tokens.update(s) # Check if match_map is provided, otherwise create it if match_map is None: matched = self.create_match_map(R, K, c_idx, inverted_index) else: matched = match_map.get(c_idx, {}) # Step 1: initialize total estimate total = total_init # Step 2: for matched r\u1d62, computational reuse of sim and adjust total if matched: for r_idx, sim in matched.items(): r_i = r_i_list[r_idx] if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) total += sim - base_loss # Step 3: for non-matched r\u1d62, compute NN and adjust total for r_idx in set(range(n)) - matched.keys(): r_i = r_i_list[r_idx] if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) r_set = set(r_i) # Case alpha > 0 if (self.alpha > 0 and len(k_i) >= floor((1 - self.alpha) * len(r_i)) + 1 and k_i.isdisjoint(S_tokens)): nn_sim = 0 else: if is_edit: # brute\u2011force search over q\u2011gram lists for edit_similarity r_list = r_i_list[r_idx] nn_sim = 0.0 for s_list in S: sim = self.similarity(r_list, s_list, self.alpha) if sim > nn_sim: nn_sim = sim else: # inverted\u2010index search for jaccard nn_sim = self._nn_search(r_set, S, c_idx, inverted_index) total += nn_sim - base_loss if total < theta: break if total >= theta: final_filtered.add(c_idx) return final_filtered verify_size(ref_size, src_size) Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Parameters: Name Type Description Default ref_size int Size of set R. required src_size int Size of (possible) set S. required Returns: Name Type Description bool bool True if both sets could be related based on their size, False otherwise. Source code in src/silkmoth/candidate_selector.py def verify_size(self, ref_size, src_size) -> bool: \"\"\" Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Args: ref_size (int): Size of set R. src_size (int): Size of (possible) set S. Returns: bool: True if both sets could be related based on their size, False otherwise. \"\"\" # case 1: Set-Containment if self.sim_metric == contain and ref_size > src_size: return False # case 2: Set-Similarity if self.sim_metric == similar: if min(ref_size, src_size) < self.delta * max(ref_size, src_size): return False return True","title":"Candidate Selector"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector","text":"The candidate selector executes the candidate selection step in the SilkMoth pipeline. After signature generation SilkMoth accesses the inverted index to get all sets which contain a token in the signature to form an initial set of candidates. The size of the candidate set can be reduced further by applying the check or nearest neighbour filters in the refinement step of the SilkMoth pipeline.","title":"CandidateSelector"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector--examples","text":">>> from silkmoth.candidate_selector import CandidateSelector >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Something\", \"Else\"}] >>> S3 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2, S3] >>> signature = [\"Apple\", \"Berlin\"] >>> I = InvertedIndex(S) >>> cand_selector = CandidateSelector(jaccard_similarity, similar, 0.7) >>> cand_selector.get_candidates(signature, I, 2) {0, 2} Source code in src/silkmoth/candidate_selector.py class CandidateSelector: \"\"\" The candidate selector executes the candidate selection step in the SilkMoth pipeline. After signature generation SilkMoth accesses the inverted index to get all sets which contain a token in the signature to form an initial set of candidates. The size of the candidate set can be reduced further by applying the check or nearest neighbour filters in the refinement step of the SilkMoth pipeline. Examples -------- ``` >>> from silkmoth.candidate_selector import CandidateSelector >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Something\", \"Else\"}] >>> S3 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2, S3] >>> signature = [\"Apple\", \"Berlin\"] >>> I = InvertedIndex(S) >>> cand_selector = CandidateSelector(jaccard_similarity, similar, 0.7) >>> cand_selector.get_candidates(signature, I, 2) {0, 2} ``` \"\"\" def __init__(self, similarity_func, sim_metric, related_thresh, sim_thresh=0.0, q = 3): \"\"\" Initialize the candidate selector with some parameters. Args: similarity_func (callable): Similarity function phi(r, s) (e.g., Jaccard). sim_metric (callable): Similarity metric related(R, S) (e.g., contain). related_thresh (float): Relatedness threshold delta. sim_thresh (float): Similarity threshold alpha. q (int): q-chunk length for edit similarity. \"\"\" self.similarity = similarity_func self.sim_metric = sim_metric self.delta = related_thresh self.alpha = sim_thresh self.q = q def get_candidates(self, signature, inverted_index, ref_size) -> set: \"\"\" Retrieve candidate set indices using token signature lookup. Args: signature (list): Signature tokens for a reference set. inverted_index (InvertedIndex): Instance of the custom InvertedIndex class. ref_size (int): Size of set R. Returns: set: Indices of candidate sets containing at least one signature token. \"\"\" candidates = set() for token in signature: try: idx_list = inverted_index.get_indexes(token) for set_idx, _ in idx_list: src_size = len(inverted_index.get_set(set_idx)) if self.verify_size(ref_size, src_size): candidates.add(set_idx) except ValueError: # token not found in inverted index; safely ignore continue return candidates def verify_size(self, ref_size, src_size) -> bool: \"\"\" Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Args: ref_size (int): Size of set R. src_size (int): Size of (possible) set S. Returns: bool: True if both sets could be related based on their size, False otherwise. \"\"\" # case 1: Set-Containment if self.sim_metric == contain and ref_size > src_size: return False # case 2: Set-Similarity if self.sim_metric == similar: if min(ref_size, src_size) < self.delta * max(ref_size, src_size): return False return True def check_filter(self, R, K, candidates, inverted_index) -> tuple: \"\"\" Apply check filter to prune weak candidate sets. Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate set indices from get_candidates(). inverted_index (InvertedIndex): For retrieving sets. Returns: tuple: set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. \"\"\" filtered = set() match_map = dict() k_i_sets = [set(r_i).intersection(K) for r_i in R] for c_idx in candidates: matched = self.create_match_map(R, k_i_sets, c_idx, inverted_index) if matched: filtered.add(c_idx) match_map[c_idx] = matched return filtered, match_map def create_match_map(self, R, k_i_sets, c_idx, inverted_index) -> dict: \"\"\" Create a match map for a specific candidate index. Args: R (list of list): Tokenized reference set. k_i_sets (list of sets): Unflattened signature. c_idx (int): Candidate set index. inverted_index (InvertedIndex): For retrieving sets. Returns: dict: r_idx -> max_sim for matched reference sets. \"\"\" S = inverted_index.get_set(c_idx) matched = {} for r_idx, (r_i, k_i) in enumerate(zip(R, k_i_sets)): if not r_i or not k_i: continue denominator = len(r_i) threshold = (denominator - len(k_i)) / denominator if denominator != 0 else 0.0 is_edit = self.similarity in (edit_similarity, N_edit_similarity) # for Jaccard set is needed, for edit list is needed if not is_edit: r_set = set(r_i) max_sim = 0.0 for token in k_i: try: entries = inverted_index.get_indexes_binary(token, c_idx) for s_idx, e_idx in entries: if s_idx != c_idx: continue s = S[e_idx] # call signature based on edit vs. jaccard if is_edit: sim = self.similarity(r_i, s, self.alpha) else: sim = self.similarity(r_set, set(s), self.alpha) if sim >= threshold: max_sim = max(max_sim, sim) except ValueError: continue if max_sim >= threshold: matched[r_idx] = max_sim return matched def _nn_search(self, r_elem, S, c_idx, inverted_index) -> float: \"\"\" Find the maximum similarity between r and elements s \u2208 S[C] that share at least one token with r using the inverted index for efficiency. Args: r_set (set): Reference element tokens. S (list of list): Elements of candidate set S[c_idx]. c_idx (int): Index of candidate set in inverted index. inverted_index (InvertedIndex): For fetching token locations. Returns: float: Maximum similarity between r and any s \u2208 S[c_idx]. \"\"\" # seen = set() max_sim = 0.0 is_edit = self.similarity in (edit_similarity, N_edit_similarity) for token in r_elem: try: entries = inverted_index.get_indexes_binary(token,c_idx) for s_idx, e_idx in entries: if s_idx != c_idx: continue s = S[e_idx] if is_edit: sim = self.similarity(r_elem, s, self.alpha) else: sim = self.similarity(set(r_elem), set(s), self.alpha) max_sim = max(max_sim, sim) except ValueError: continue return max_sim def nn_filter(self, R, K, candidates, inverted_index, threshold, match_map) -> set: \"\"\" Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate indices from check filter. inverted_index (InvertedIndex): To retrieve sets and indexes. threshold (float): Relatedness threshold \u03b4 (between 0 and 1). match_map (dict): Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). Returns: set: Final filtered candidate indices that pass the NN filter. \"\"\" n = len(R) theta = threshold * n is_edit = self.similarity in (edit_similarity, N_edit_similarity) k_i_sets = [set(r_i).intersection(K) for r_i in R] r_i_list = R final_filtered = set() total_init = 0 for r_idx, r_i in enumerate(R): if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) total_init += base_loss for c_idx in candidates: S = inverted_index.get_set(c_idx) if self.alpha > 0: S_tokens = set() for s in S: S_tokens.update(s) # Check if match_map is provided, otherwise create it if match_map is None: matched = self.create_match_map(R, K, c_idx, inverted_index) else: matched = match_map.get(c_idx, {}) # Step 1: initialize total estimate total = total_init # Step 2: for matched r\u1d62, computational reuse of sim and adjust total if matched: for r_idx, sim in matched.items(): r_i = r_i_list[r_idx] if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) total += sim - base_loss # Step 3: for non-matched r\u1d62, compute NN and adjust total for r_idx in set(range(n)) - matched.keys(): r_i = r_i_list[r_idx] if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) r_set = set(r_i) # Case alpha > 0 if (self.alpha > 0 and len(k_i) >= floor((1 - self.alpha) * len(r_i)) + 1 and k_i.isdisjoint(S_tokens)): nn_sim = 0 else: if is_edit: # brute\u2011force search over q\u2011gram lists for edit_similarity r_list = r_i_list[r_idx] nn_sim = 0.0 for s_list in S: sim = self.similarity(r_list, s_list, self.alpha) if sim > nn_sim: nn_sim = sim else: # inverted\u2010index search for jaccard nn_sim = self._nn_search(r_set, S, c_idx, inverted_index) total += nn_sim - base_loss if total < theta: break if total >= theta: final_filtered.add(c_idx) return final_filtered def calc_base_loss(self, k_i, r_i): if self.similarity in (edit_similarity, N_edit_similarity): denominator = len(r_i) + len(k_i) base_loss = len(r_i) / denominator if denominator != 0 else 0.0 else: denominator = len(r_i) base_loss = (len(r_i) - len(k_i)) / denominator if denominator != 0 else 0.0 return base_loss","title":"Examples"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.__init__","text":"Initialize the candidate selector with some parameters. Parameters: Name Type Description Default similarity_func callable Similarity function phi(r, s) (e.g., Jaccard). required sim_metric callable Similarity metric related(R, S) (e.g., contain). required related_thresh float Relatedness threshold delta. required sim_thresh float Similarity threshold alpha. 0.0 q int q-chunk length for edit similarity. 3 Source code in src/silkmoth/candidate_selector.py def __init__(self, similarity_func, sim_metric, related_thresh, sim_thresh=0.0, q = 3): \"\"\" Initialize the candidate selector with some parameters. Args: similarity_func (callable): Similarity function phi(r, s) (e.g., Jaccard). sim_metric (callable): Similarity metric related(R, S) (e.g., contain). related_thresh (float): Relatedness threshold delta. sim_thresh (float): Similarity threshold alpha. q (int): q-chunk length for edit similarity. \"\"\" self.similarity = similarity_func self.sim_metric = sim_metric self.delta = related_thresh self.alpha = sim_thresh self.q = q","title":"__init__"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.check_filter","text":"Apply check filter to prune weak candidate sets. Parameters: Name Type Description Default R list of list Tokenized reference set. required K set Flattened signature tokens. required candidates set Candidate set indices from get_candidates(). required inverted_index InvertedIndex For retrieving sets. required Returns: Name Type Description tuple tuple set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. Source code in src/silkmoth/candidate_selector.py def check_filter(self, R, K, candidates, inverted_index) -> tuple: \"\"\" Apply check filter to prune weak candidate sets. Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate set indices from get_candidates(). inverted_index (InvertedIndex): For retrieving sets. Returns: tuple: set: Candidate indices that pass the check filter. dict: c_idx -> dict{r_idx -> max_sim}. \"\"\" filtered = set() match_map = dict() k_i_sets = [set(r_i).intersection(K) for r_i in R] for c_idx in candidates: matched = self.create_match_map(R, k_i_sets, c_idx, inverted_index) if matched: filtered.add(c_idx) match_map[c_idx] = matched return filtered, match_map","title":"check_filter"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.create_match_map","text":"Create a match map for a specific candidate index. Parameters: Name Type Description Default R list of list Tokenized reference set. required k_i_sets list of sets Unflattened signature. required c_idx int Candidate set index. required inverted_index InvertedIndex For retrieving sets. required Returns: Name Type Description dict dict r_idx -> max_sim for matched reference sets. Source code in src/silkmoth/candidate_selector.py def create_match_map(self, R, k_i_sets, c_idx, inverted_index) -> dict: \"\"\" Create a match map for a specific candidate index. Args: R (list of list): Tokenized reference set. k_i_sets (list of sets): Unflattened signature. c_idx (int): Candidate set index. inverted_index (InvertedIndex): For retrieving sets. Returns: dict: r_idx -> max_sim for matched reference sets. \"\"\" S = inverted_index.get_set(c_idx) matched = {} for r_idx, (r_i, k_i) in enumerate(zip(R, k_i_sets)): if not r_i or not k_i: continue denominator = len(r_i) threshold = (denominator - len(k_i)) / denominator if denominator != 0 else 0.0 is_edit = self.similarity in (edit_similarity, N_edit_similarity) # for Jaccard set is needed, for edit list is needed if not is_edit: r_set = set(r_i) max_sim = 0.0 for token in k_i: try: entries = inverted_index.get_indexes_binary(token, c_idx) for s_idx, e_idx in entries: if s_idx != c_idx: continue s = S[e_idx] # call signature based on edit vs. jaccard if is_edit: sim = self.similarity(r_i, s, self.alpha) else: sim = self.similarity(r_set, set(s), self.alpha) if sim >= threshold: max_sim = max(max_sim, sim) except ValueError: continue if max_sim >= threshold: matched[r_idx] = max_sim return matched","title":"create_match_map"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.get_candidates","text":"Retrieve candidate set indices using token signature lookup. Parameters: Name Type Description Default signature list Signature tokens for a reference set. required inverted_index InvertedIndex Instance of the custom InvertedIndex class. required ref_size int Size of set R. required Returns: Name Type Description set set Indices of candidate sets containing at least one signature token. Source code in src/silkmoth/candidate_selector.py def get_candidates(self, signature, inverted_index, ref_size) -> set: \"\"\" Retrieve candidate set indices using token signature lookup. Args: signature (list): Signature tokens for a reference set. inverted_index (InvertedIndex): Instance of the custom InvertedIndex class. ref_size (int): Size of set R. Returns: set: Indices of candidate sets containing at least one signature token. \"\"\" candidates = set() for token in signature: try: idx_list = inverted_index.get_indexes(token) for set_idx, _ in idx_list: src_size = len(inverted_index.get_set(set_idx)) if self.verify_size(ref_size, src_size): candidates.add(set_idx) except ValueError: # token not found in inverted index; safely ignore continue return candidates","title":"get_candidates"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.nn_filter","text":"Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Parameters: Name Type Description Default R list of list Tokenized reference set. required K set Flattened signature tokens. required candidates set Candidate indices from check filter. required inverted_index InvertedIndex To retrieve sets and indexes. required threshold float Relatedness threshold \u03b4 (between 0 and 1). required match_map dict Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). required Returns: Name Type Description set set Final filtered candidate indices that pass the NN filter. Source code in src/silkmoth/candidate_selector.py def nn_filter(self, R, K, candidates, inverted_index, threshold, match_map) -> set: \"\"\" Nearest Neighbor Filter (Algorithm 2 from SilkMoth paper). Args: R (list of list): Tokenized reference set. K (set): Flattened signature tokens. candidates (set): Candidate indices from check filter. inverted_index (InvertedIndex): To retrieve sets and indexes. threshold (float): Relatedness threshold \u03b4 (between 0 and 1). match_map (dict): Maps candidate set index to matched r\u1d62 indices and their max sim (from check filter). Returns: set: Final filtered candidate indices that pass the NN filter. \"\"\" n = len(R) theta = threshold * n is_edit = self.similarity in (edit_similarity, N_edit_similarity) k_i_sets = [set(r_i).intersection(K) for r_i in R] r_i_list = R final_filtered = set() total_init = 0 for r_idx, r_i in enumerate(R): if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) total_init += base_loss for c_idx in candidates: S = inverted_index.get_set(c_idx) if self.alpha > 0: S_tokens = set() for s in S: S_tokens.update(s) # Check if match_map is provided, otherwise create it if match_map is None: matched = self.create_match_map(R, K, c_idx, inverted_index) else: matched = match_map.get(c_idx, {}) # Step 1: initialize total estimate total = total_init # Step 2: for matched r\u1d62, computational reuse of sim and adjust total if matched: for r_idx, sim in matched.items(): r_i = r_i_list[r_idx] if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) total += sim - base_loss # Step 3: for non-matched r\u1d62, compute NN and adjust total for r_idx in set(range(n)) - matched.keys(): r_i = r_i_list[r_idx] if not r_i: continue k_i = k_i_sets[r_idx] base_loss = self.calc_base_loss(k_i, r_i) r_set = set(r_i) # Case alpha > 0 if (self.alpha > 0 and len(k_i) >= floor((1 - self.alpha) * len(r_i)) + 1 and k_i.isdisjoint(S_tokens)): nn_sim = 0 else: if is_edit: # brute\u2011force search over q\u2011gram lists for edit_similarity r_list = r_i_list[r_idx] nn_sim = 0.0 for s_list in S: sim = self.similarity(r_list, s_list, self.alpha) if sim > nn_sim: nn_sim = sim else: # inverted\u2010index search for jaccard nn_sim = self._nn_search(r_set, S, c_idx, inverted_index) total += nn_sim - base_loss if total < theta: break if total >= theta: final_filtered.add(c_idx) return final_filtered","title":"nn_filter"},{"location":"pages/candidate_selector/#silkmoth.candidate_selector.CandidateSelector.verify_size","text":"Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Parameters: Name Type Description Default ref_size int Size of set R. required src_size int Size of (possible) set S. required Returns: Name Type Description bool bool True if both sets could be related based on their size, False otherwise. Source code in src/silkmoth/candidate_selector.py def verify_size(self, ref_size, src_size) -> bool: \"\"\" Checks if sets can be related based on their sizes. Set-Containment is only defined for |R|<=|S|. For Set-Similarity we should compare only similar size sets. Args: ref_size (int): Size of set R. src_size (int): Size of (possible) set S. Returns: bool: True if both sets could be related based on their size, False otherwise. \"\"\" # case 1: Set-Containment if self.sim_metric == contain and ref_size > src_size: return False # case 2: Set-Similarity if self.sim_metric == similar: if min(ref_size, src_size) < self.delta * max(ref_size, src_size): return False return True","title":"verify_size"},{"location":"pages/inverted_index/","text":"InvertedIndex The inverted index allows to lookup all appearances of a token in a collection of tokenized sets returns inverted lists consisting of (set, element) tuples supports full sets/elements and positional indexes of them stores source sets in SilkMothEngine The inverted list is sorted first by the order of the sets and then by the order of the elements. Examples >>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> I.get_indexes(\"Sun\") [(0, 1), (1, 0)] >>> I[\"Berlin\"] [([{'Sun', 'Apple', 'Berlin'}, {'Apple'}], {'Sun', 'Apple', 'Berlin'})] SilkMoth Inverted Index. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0. Source code in src/silkmoth/inverted_index.py class InvertedIndex: \"\"\" The inverted index - allows to lookup all appearances of a token in a collection of tokenized sets - returns inverted lists consisting of (set, element) tuples - supports full sets/elements and positional indexes of them - stores source sets in [SilkMothEngine](silkmoth_engine.md) The inverted list - is sorted first by the order of the sets and then by the order of the elements. Examples -------- ``` >>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> I.get_indexes(\"Sun\") [(0, 1), (1, 0)] >>> I[\"Berlin\"] [([{'Sun', 'Apple', 'Berlin'}, {'Apple'}], {'Sun', 'Apple', 'Berlin'})] ``` ![SilkMoth Inverted Index](../figures/InvertedIndex.png) *SilkMoth Inverted Index. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0.* \"\"\" def __init__(self, token_sets: list): \"\"\" Initialize the inverted index. Args: token_sets (list): Collection of tokenized sets sim_func (function): Similarity function \"\"\" #self.sim_func = sim_func self.token_sets = [] self.lookup_table = dict() for set_idx, token_set in enumerate(token_sets): self.token_sets.append(token_set) for element_idx, tokens in enumerate(token_set): for token in tokens: key = (set_idx, element_idx) if token not in self.lookup_table: self.lookup_table[token] = [key] elif self.lookup_table[token][-1] != key: self.lookup_table[token].append(key) def keys(self): \"\"\" Gives all tokens similar like dict.keys(). Returns: set (set): A set-like object providing all keys \"\"\" return self.lookup_table.keys() def __getitem__(self, token) -> list: \"\"\" Access inverted list from inverted index using square brackets. Args: token (str): Input token Returns: list: A list of all (set, element) tuples which contain the input token. \"\"\" idx_list = self.get_indexes(token) return [(self.get_set(s), self.get_set(s)[e]) for s, e in idx_list] def get_indexes(self, token) -> list: \"\"\" Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Args: token (str): Input token Returns: list: A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple \"\"\" if not token in self.lookup_table: raise ValueError(f\"Unknown token\") return self.lookup_table[token] def get_set(self, set_id: int) -> list: \"\"\" Access (tokenized) set from set ID. Args: set_id: Set ID Returns: list: Tokenized set \"\"\" if set_id < 0 or set_id >= len(self.token_sets): raise ValueError(f\"Invalid id\") return self.token_sets[set_id] def get_indexes_binary(self, token, set_idx) -> list: \"\"\" Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Args: token (str): The token to search in the inverted index. set_idx (int): The ID of the set we want the element indexes for. Returns: list: All (set_idx, element_idx) tuples where the token appears in the given set. \"\"\" if token not in self.lookup_table: raise ValueError(\"Unknown token\") index_list = self.lookup_table[token] # Using bisect to find the range of entries where set_idx matches left = bisect.bisect_left(index_list, (set_idx, -1)) right = bisect.bisect_right(index_list, (set_idx, float('inf'))) return index_list[left:right] def print_index(self): \"\"\" Prints the inverted index in a readable format. \"\"\" print(\"=== Inverted Index ===\") for token, locations in self.lookup_table.items(): print(f\"Token: {token} \u2192 Locations: {locations}\") __getitem__(token) Access inverted list from inverted index using square brackets. Parameters: Name Type Description Default token str Input token required Returns: Name Type Description list list A list of all (set, element) tuples which contain the input token. Source code in src/silkmoth/inverted_index.py def __getitem__(self, token) -> list: \"\"\" Access inverted list from inverted index using square brackets. Args: token (str): Input token Returns: list: A list of all (set, element) tuples which contain the input token. \"\"\" idx_list = self.get_indexes(token) return [(self.get_set(s), self.get_set(s)[e]) for s, e in idx_list] __init__(token_sets) Initialize the inverted index. Parameters: Name Type Description Default token_sets list Collection of tokenized sets required sim_func function Similarity function required Source code in src/silkmoth/inverted_index.py def __init__(self, token_sets: list): \"\"\" Initialize the inverted index. Args: token_sets (list): Collection of tokenized sets sim_func (function): Similarity function \"\"\" #self.sim_func = sim_func self.token_sets = [] self.lookup_table = dict() for set_idx, token_set in enumerate(token_sets): self.token_sets.append(token_set) for element_idx, tokens in enumerate(token_set): for token in tokens: key = (set_idx, element_idx) if token not in self.lookup_table: self.lookup_table[token] = [key] elif self.lookup_table[token][-1] != key: self.lookup_table[token].append(key) get_indexes(token) Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Parameters: Name Type Description Default token str Input token required Returns: Name Type Description list list A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple Source code in src/silkmoth/inverted_index.py def get_indexes(self, token) -> list: \"\"\" Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Args: token (str): Input token Returns: list: A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple \"\"\" if not token in self.lookup_table: raise ValueError(f\"Unknown token\") return self.lookup_table[token] get_indexes_binary(token, set_idx) Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Parameters: Name Type Description Default token str The token to search in the inverted index. required set_idx int The ID of the set we want the element indexes for. required Returns: Name Type Description list list All (set_idx, element_idx) tuples where the token appears in the given set. Source code in src/silkmoth/inverted_index.py def get_indexes_binary(self, token, set_idx) -> list: \"\"\" Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Args: token (str): The token to search in the inverted index. set_idx (int): The ID of the set we want the element indexes for. Returns: list: All (set_idx, element_idx) tuples where the token appears in the given set. \"\"\" if token not in self.lookup_table: raise ValueError(\"Unknown token\") index_list = self.lookup_table[token] # Using bisect to find the range of entries where set_idx matches left = bisect.bisect_left(index_list, (set_idx, -1)) right = bisect.bisect_right(index_list, (set_idx, float('inf'))) return index_list[left:right] get_set(set_id) Access (tokenized) set from set ID. Parameters: Name Type Description Default set_id int Set ID required Returns: Name Type Description list list Tokenized set Source code in src/silkmoth/inverted_index.py def get_set(self, set_id: int) -> list: \"\"\" Access (tokenized) set from set ID. Args: set_id: Set ID Returns: list: Tokenized set \"\"\" if set_id < 0 or set_id >= len(self.token_sets): raise ValueError(f\"Invalid id\") return self.token_sets[set_id] keys() Gives all tokens similar like dict.keys(). Returns: Name Type Description set set A set-like object providing all keys Source code in src/silkmoth/inverted_index.py def keys(self): \"\"\" Gives all tokens similar like dict.keys(). Returns: set (set): A set-like object providing all keys \"\"\" return self.lookup_table.keys() print_index() Prints the inverted index in a readable format. Source code in src/silkmoth/inverted_index.py def print_index(self): \"\"\" Prints the inverted index in a readable format. \"\"\" print(\"=== Inverted Index ===\") for token, locations in self.lookup_table.items(): print(f\"Token: {token} \u2192 Locations: {locations}\")","title":"Inverted Index"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex","text":"The inverted index allows to lookup all appearances of a token in a collection of tokenized sets returns inverted lists consisting of (set, element) tuples supports full sets/elements and positional indexes of them stores source sets in SilkMothEngine The inverted list is sorted first by the order of the sets and then by the order of the elements.","title":"InvertedIndex"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex--examples","text":">>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> I.get_indexes(\"Sun\") [(0, 1), (1, 0)] >>> I[\"Berlin\"] [([{'Sun', 'Apple', 'Berlin'}, {'Apple'}], {'Sun', 'Apple', 'Berlin'})] SilkMoth Inverted Index. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0. Source code in src/silkmoth/inverted_index.py class InvertedIndex: \"\"\" The inverted index - allows to lookup all appearances of a token in a collection of tokenized sets - returns inverted lists consisting of (set, element) tuples - supports full sets/elements and positional indexes of them - stores source sets in [SilkMothEngine](silkmoth_engine.md) The inverted list - is sorted first by the order of the sets and then by the order of the elements. Examples -------- ``` >>> from silkmoth.inverted_index import InvertedIndex >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> I.get_indexes(\"Sun\") [(0, 1), (1, 0)] >>> I[\"Berlin\"] [([{'Sun', 'Apple', 'Berlin'}, {'Apple'}], {'Sun', 'Apple', 'Berlin'})] ``` ![SilkMoth Inverted Index](../figures/InvertedIndex.png) *SilkMoth Inverted Index. Source: Deng et al., \"SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints\", VLDB 2017. Licensed under CC BY-NC-ND 4.0.* \"\"\" def __init__(self, token_sets: list): \"\"\" Initialize the inverted index. Args: token_sets (list): Collection of tokenized sets sim_func (function): Similarity function \"\"\" #self.sim_func = sim_func self.token_sets = [] self.lookup_table = dict() for set_idx, token_set in enumerate(token_sets): self.token_sets.append(token_set) for element_idx, tokens in enumerate(token_set): for token in tokens: key = (set_idx, element_idx) if token not in self.lookup_table: self.lookup_table[token] = [key] elif self.lookup_table[token][-1] != key: self.lookup_table[token].append(key) def keys(self): \"\"\" Gives all tokens similar like dict.keys(). Returns: set (set): A set-like object providing all keys \"\"\" return self.lookup_table.keys() def __getitem__(self, token) -> list: \"\"\" Access inverted list from inverted index using square brackets. Args: token (str): Input token Returns: list: A list of all (set, element) tuples which contain the input token. \"\"\" idx_list = self.get_indexes(token) return [(self.get_set(s), self.get_set(s)[e]) for s, e in idx_list] def get_indexes(self, token) -> list: \"\"\" Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Args: token (str): Input token Returns: list: A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple \"\"\" if not token in self.lookup_table: raise ValueError(f\"Unknown token\") return self.lookup_table[token] def get_set(self, set_id: int) -> list: \"\"\" Access (tokenized) set from set ID. Args: set_id: Set ID Returns: list: Tokenized set \"\"\" if set_id < 0 or set_id >= len(self.token_sets): raise ValueError(f\"Invalid id\") return self.token_sets[set_id] def get_indexes_binary(self, token, set_idx) -> list: \"\"\" Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Args: token (str): The token to search in the inverted index. set_idx (int): The ID of the set we want the element indexes for. Returns: list: All (set_idx, element_idx) tuples where the token appears in the given set. \"\"\" if token not in self.lookup_table: raise ValueError(\"Unknown token\") index_list = self.lookup_table[token] # Using bisect to find the range of entries where set_idx matches left = bisect.bisect_left(index_list, (set_idx, -1)) right = bisect.bisect_right(index_list, (set_idx, float('inf'))) return index_list[left:right] def print_index(self): \"\"\" Prints the inverted index in a readable format. \"\"\" print(\"=== Inverted Index ===\") for token, locations in self.lookup_table.items(): print(f\"Token: {token} \u2192 Locations: {locations}\")","title":"Examples"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.__getitem__","text":"Access inverted list from inverted index using square brackets. Parameters: Name Type Description Default token str Input token required Returns: Name Type Description list list A list of all (set, element) tuples which contain the input token. Source code in src/silkmoth/inverted_index.py def __getitem__(self, token) -> list: \"\"\" Access inverted list from inverted index using square brackets. Args: token (str): Input token Returns: list: A list of all (set, element) tuples which contain the input token. \"\"\" idx_list = self.get_indexes(token) return [(self.get_set(s), self.get_set(s)[e]) for s, e in idx_list]","title":"__getitem__"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.__init__","text":"Initialize the inverted index. Parameters: Name Type Description Default token_sets list Collection of tokenized sets required sim_func function Similarity function required Source code in src/silkmoth/inverted_index.py def __init__(self, token_sets: list): \"\"\" Initialize the inverted index. Args: token_sets (list): Collection of tokenized sets sim_func (function): Similarity function \"\"\" #self.sim_func = sim_func self.token_sets = [] self.lookup_table = dict() for set_idx, token_set in enumerate(token_sets): self.token_sets.append(token_set) for element_idx, tokens in enumerate(token_set): for token in tokens: key = (set_idx, element_idx) if token not in self.lookup_table: self.lookup_table[token] = [key] elif self.lookup_table[token][-1] != key: self.lookup_table[token].append(key)","title":"__init__"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.get_indexes","text":"Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Parameters: Name Type Description Default token str Input token required Returns: Name Type Description list list A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple Source code in src/silkmoth/inverted_index.py def get_indexes(self, token) -> list: \"\"\" Access inverted list of indexes. For some tasks retrieving the full set and element pairs might not be necessary and their indexes are sufficient. Args: token (str): Input token Returns: list: A list of all (set index, element index) tuples for (set, element) tuples which contain the input tuple \"\"\" if not token in self.lookup_table: raise ValueError(f\"Unknown token\") return self.lookup_table[token]","title":"get_indexes"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.get_indexes_binary","text":"Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Parameters: Name Type Description Default token str The token to search in the inverted index. required set_idx int The ID of the set we want the element indexes for. required Returns: Name Type Description list list All (set_idx, element_idx) tuples where the token appears in the given set. Source code in src/silkmoth/inverted_index.py def get_indexes_binary(self, token, set_idx) -> list: \"\"\" Uses binary search to get all (set_idx, element_idx) pairs for a token where set_idx matches the given set_idx. Args: token (str): The token to search in the inverted index. set_idx (int): The ID of the set we want the element indexes for. Returns: list: All (set_idx, element_idx) tuples where the token appears in the given set. \"\"\" if token not in self.lookup_table: raise ValueError(\"Unknown token\") index_list = self.lookup_table[token] # Using bisect to find the range of entries where set_idx matches left = bisect.bisect_left(index_list, (set_idx, -1)) right = bisect.bisect_right(index_list, (set_idx, float('inf'))) return index_list[left:right]","title":"get_indexes_binary"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.get_set","text":"Access (tokenized) set from set ID. Parameters: Name Type Description Default set_id int Set ID required Returns: Name Type Description list list Tokenized set Source code in src/silkmoth/inverted_index.py def get_set(self, set_id: int) -> list: \"\"\" Access (tokenized) set from set ID. Args: set_id: Set ID Returns: list: Tokenized set \"\"\" if set_id < 0 or set_id >= len(self.token_sets): raise ValueError(f\"Invalid id\") return self.token_sets[set_id]","title":"get_set"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.keys","text":"Gives all tokens similar like dict.keys(). Returns: Name Type Description set set A set-like object providing all keys Source code in src/silkmoth/inverted_index.py def keys(self): \"\"\" Gives all tokens similar like dict.keys(). Returns: set (set): A set-like object providing all keys \"\"\" return self.lookup_table.keys()","title":"keys"},{"location":"pages/inverted_index/#silkmoth.inverted_index.InvertedIndex.print_index","text":"Prints the inverted index in a readable format. Source code in src/silkmoth/inverted_index.py def print_index(self): \"\"\" Prints the inverted index in a readable format. \"\"\" print(\"=== Inverted Index ===\") for token, locations in self.lookup_table.items(): print(f\"Token: {token} \u2192 Locations: {locations}\")","title":"print_index"},{"location":"pages/signature_generator/","text":"SignatureGenerator The signature generator executes the signature generation step in the SilkMoth pipeline. During signature generation SilkMoth constructs a signature for a reference set R by selecting the \u201csmallest\u201d set of tokens from R such that if another set S does not share any tokens with R's signature, R and S are not related. SilkMoth supports three different signature schemes: Weighted Signature Scheme (Def. 5 in paper ), Skyline Signature Scheme (Def. 9), and Dichotomy Signature Scheme (Def. 10). Examples >>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.signature_generator import SignatureGenerator >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Sun\", \"Berlin\", \"Paris\"}] >>> sig_gen = SignatureGenerator() >>> sig_gen.get_signature(R, I, 0.3) ['Apple', 'Sun', 'Berlin'] >>> sig_gen.get_signature(R, I, 0.5) ['Apple', 'Berlin'] Source code in src/silkmoth/signature_generator.py class SignatureGenerator: \"\"\" The signature generator executes the signature generation step in the SilkMoth pipeline. During signature generation SilkMoth constructs a signature for a reference set R by selecting the \u201csmallest\u201d set of tokens from R such that if another set S does not share any tokens with R's signature, R and S are not related. SilkMoth supports three different signature schemes: Weighted Signature Scheme (Def. 5 in [paper](https://doi.org/10.14778/3115404.3115413)), Skyline Signature Scheme (Def. 9), and Dichotomy Signature Scheme (Def. 10). Examples -------- ``` >>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.signature_generator import SignatureGenerator >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Sun\", \"Berlin\", \"Paris\"}] >>> sig_gen = SignatureGenerator() >>> sig_gen.get_signature(R, I, 0.3) ['Apple', 'Sun', 'Berlin'] >>> sig_gen.get_signature(R, I, 0.5) ['Apple', 'Berlin'] ``` \"\"\" def __init__(self): \"\"\" Initialize the signature generator with default parameters. \"\"\" self.sim_fun = jaccard_similarity self.q = 3 def get_signature(self, reference_set, inverted_index, delta, alpha=0, sig_type=SigType.WEIGHTED, sim_fun = jaccard_similarity, q=3) -> list: \"\"\" Compute a signature for a reference set according to the chosen signature type. Args: reference_set (list): Tokenized reference set. inverted_index (InvertedIndex): Index to evaluate token cost. delta (float): Relatedness threshold factor. alpha (float): Similarity threshold factor. sig_type (SigType): Type of signature. sim_fun (callable): Similarity function (phi) to use. q (int): Length of each q-chunk for edit similarity. Returns: list: A list of str for selected tokens forming the signature. \"\"\" if sim_fun in (edit_similarity, N_edit_similarity): self.sim_fun = sim_fun self.q = q # If q is too large, no valid weighted signature exists q_bound = delta / (1 - delta) if q >= q_bound: warnings.warn( f\"q={q} is too large for delta = {delta:.3f}; \" \"no valid weighted signature exists -> falling back to brute-force.\" ) # returning all non-overlapping q-chunks so nothing is pruned all_chunks = [] for elem in reference_set: joined = \" \".join(elem) all_chunks += [ joined[i:i+q] for i in range(0, len(joined) - q + 1, q) ] return all_chunks is_edit_sim = sim_fun in (edit_similarity, N_edit_similarity) match sig_type: case SigType.WEIGHTED: if is_edit_sim and alpha > 0: return self._generate_simthresh_signature_edit_similarity(reference_set, inverted_index, delta, alpha) elif is_edit_sim and alpha == 0: return self._generate_weighted_signature_edit_similarity(reference_set, inverted_index, delta) else: return self._generate_weighted_signature(reference_set, inverted_index, delta) case SigType.SKYLINE: return self._generate_skyline_signature(reference_set, inverted_index, delta, alpha) case SigType.DICHOTOMY: return self._generate_dichotomy_signature(reference_set, inverted_index, delta, alpha) case _: raise ValueError(f\"Unknown signature type\") def _generate_simthresh_signature_edit_similarity(self, reference_set, inverted_index, delta, alpha) -> list: \"\"\" Builds a similarity-threshold signature for edit similarity as described in the SILKMOTH Paper in Section 7.2. For each element r_i in the reference set, it ensures that the signature contains at least m_i = floor((1 - alpha)/alpha * |q_chunks(r_i)|) + 1 q-chunks, so that any element sharing fewer than m_i chunks cannot achieve Eds \u2265 alpha. Args: reference_set (list): Tokenized reference set. inverted_index (InvertedIndex): Index to evaluate token cost. delta (float): Relatedness threshold factor. alpha (float): Similarity threshold factor. Returns: list: A list of str for selected q-chunks forming the signature. \"\"\" weighted_sig_edit_sim = set(self._generate_weighted_signature_edit_similarity(reference_set, inverted_index, delta)) simthresh_sig = set(weighted_sig_edit_sim) for elem_tokens in reference_set: # compute all q-chunks of each element joined = \" \".join(elem_tokens) chunks = [joined[j:j+self.q] for j in range(0, len(joined) - self.q + 1, self.q)] r = set(chunks) m_i = floor((1 - alpha) / alpha * len(r)) + 1 # determine how many base chunks from this element are already in weighted signature k_i = list(weighted_sig_edit_sim & r) if len(k_i) < m_i: # need cheapest additional chunks -> sort all chunks by cost = inverted_index size sorted_chunks = sorted( r, key=lambda t: len(inverted_index.get_indexes(t)) if t in inverted_index.lookup_table else float('inf') ) # add cheapest chunks up to m_i for chunk in sorted_chunks: if len(simthresh_sig & r) >= m_i: break simthresh_sig.add(chunk) return list(simthresh_sig) def _generate_skyline_signature(self, reference_set, inverted_index: InvertedIndex, delta, alpha): if self.sim_fun == jaccard_similarity: weighted = set(self._generate_weighted_signature(reference_set, inverted_index, delta)) unflattened = [weighted & set(r_i) for r_i in reference_set] elif self.sim_fun in (edit_similarity, N_edit_similarity): weighted = set(self._generate_weighted_signature_edit_similarity(reference_set, inverted_index, delta)) unflattened = [weighted & set(\" \".join(r_i)[i:i+self.q] for i in range(0, len(\" \".join(r_i)) - self.q + 1, self.q)) for r_i in reference_set] else: raise ValueError(f\"Unknown similarity function: {self.sim_fun}\") skyline = set() for i, k in enumerate(unflattened): if self.sim_fun == jaccard_similarity: rhs = floor((1 - alpha) * len(reference_set[i])) + 1 elif self.sim_fun in (edit_similarity, N_edit_similarity): chunks = get_q_chunks(reference_set[i], self.q) r = set(chunks) rhs = floor((1 - alpha) / alpha * len(r)) + 1 else: raise ValueError(f\"Unknown similarity function: {self.sim_fun}\") if len(k) < rhs: skyline |= k else: # add tokens with minimum |I[t]| tokens = list(k) tokens.sort(key=lambda t: len(inverted_index.get_indexes(t))) skyline = skyline.union(tokens[:rhs]) return list(skyline) def _generate_dichotomy_signature(self, reference_set, inverted_index: InvertedIndex, delta, alpha): \"\"\" Generates a signature using the Dichotomy Scheme as described in the SILKMOTH paper. For each element r_i, it chooses between its weighted signature part (k_i) and all its tokens (r_i) based on whether k_i is a subset of an optimal sim-thresh signature (m_i). \"\"\" # 1. First, generate the optimal weighted signature, K. if self.sim_fun == jaccard_similarity: weighted_signature_K = set(self._generate_weighted_signature(reference_set, inverted_index, delta)) elif self.sim_fun in (edit_similarity, N_edit_similarity): weighted_signature_K = set(self._generate_weighted_signature_edit_similarity(reference_set, inverted_index, delta)) else: raise ValueError(f\"Unknown similarity function: {self.sim_fun}\") final_dichotomy_sig = set() # 2. For each element r_i, decide whether to use its k_i or the full r_i. for r_i_list in reference_set: if self.sim_fun == jaccard_similarity: r_i = set(r_i_list) elif self.sim_fun in (edit_similarity, N_edit_similarity): chunks = get_q_chunks(r_i_list, self.q) r_i = set(chunks) else: raise ValueError(f\"Unknown similarity function: {self.sim_fun}\") if not r_i: continue # 3. Determine k_i: the part of the weighted signature in this element. k_i = weighted_signature_K.intersection(r_i) # 4. Determine m_i: the optimal sim-thresh signature for this element. # 4a. Calculate the required size for the sim-thresh signature. if self.sim_fun == jaccard_similarity: m_i_size = floor((1 - alpha) * len(r_i)) + 1 elif self.sim_fun in (edit_similarity, N_edit_similarity): m_i_size = floor((1 - alpha) / alpha * len(r_i)) + 1 else: raise ValueError(f\"Unknown similarity function: {self.sim_fun}\") # 4b. Get all tokens from the original element r_i and sort by cost. element_tokens = list(r_i) # Sort tokens by the length of their inverted index list (cost). # Handle cases where a token might not be in the index. def get_token_cost(token): try: return len(inverted_index.get_indexes(token)) except ValueError: return float('inf') # Assign a high cost if not found element_tokens.sort(key=get_token_cost) # 4c. The optimal m_i consists of the cheapest tokens. m_i = set(element_tokens[:m_i_size]) # 5. The Decision: Apply the paper's condition. # Can we get away with the cheaper weighted signature part (k_i)? # Yes, if k_i is already a subset of the optimal sim-thresh signature (m_i). if k_i.issubset(m_i): # Decision: Use the cheaper weighted signature tokens. final_dichotomy_sig.update(k_i) else: # Decision: Fall back to the safe, more expensive option. final_dichotomy_sig.update(r_i) return list(final_dichotomy_sig) def _generate_weighted_signature(self, reference_set, inverted_index, delta): if delta <= 0.0: return [] n = len(reference_set) theta = delta * n # required covered fraction , delta * |R| in paper # 1) Build token: elements map and aggregate token values token_to_elems = defaultdict(list) token_value = {} for i, elem in enumerate(reference_set): if not elem: warnings.warn(f\"Element at index {i} is empty and will be skipped.\") continue unique_tokens = set(elem) # remove duplicate tokens inside each element weight = 1.0 / len(unique_tokens) for t in unique_tokens: token_to_elems[t].append(i) token_value[t] = token_value.get(t, 0.0) + weight # value = sum of weights (for each token) # 2) Build min-heap of (cost/value, token) heap = [] for t, val in token_value.items(): if val <= 0: continue try: cost = len(inverted_index.get_indexes(t)) # look up each token in inverted index to count in how many sets it is = cost except ValueError: # Token not in index: assign infinite cost to deprioritize cost = float('inf') heapq.heappush(heap, (cost / val, t)) # goal small ratio: cost/value # 3) Selection with greedy algorithm selected_sig = set() r_sizes = [len(set(elem)) if elem else 0 for elem in reference_set] total_loss = float(n) current_k_counts = [0] * n # while heap and total_loss >= theta: while heap and total_loss >= theta: # 1. ratio, t = heapq.heappop(heap) # pull best token with lowest cost/value from heap if t in selected_sig: continue if ratio == float('inf'): break # 2. selected_sig.add(t) # 3. for i in range(n): if r_sizes[i] == 0: continue # Calculate |k_i|: number of tokens from reference_set[i] also in selected_sig current_k_counts[i] = len(set(reference_set[i]).intersection(selected_sig)) # 4. total_loss = sum( (r_sizes[i] - current_k_counts[i]) / r_sizes[i] for i in range(n) if r_sizes[i] > 0 ) return list(selected_sig) # Following the same logic of _generate_weighted_signature def _generate_weighted_signature_edit_similarity(self, reference_set, inverted_index, delta): if delta <= 0.0: return [] n = len(reference_set) theta = delta * n token_to_elems = defaultdict(list) # map q-chunk -> list of element indexes token_value = {} # map q-chunk -> accumulated value r_sizes = [] # number of q-chunks per element element_chunks = [] # list of q-chunks per element # Step 1: Build q-chunks and token values for i, elem_tokens in enumerate(reference_set): if not elem_tokens: # warnings.warn(f\"Element at index {i} is empty and will be skipped.\") r_sizes.append(0) element_chunks.append([]) continue # Join the list of tokens into a string joined = \" \".join(elem_tokens) # Extract non-overlapping q-chunks chunks = [joined[j:j+self.q] for j in range(0, len(joined) - self.q + 1, self.q)] chunk_set = set(chunks) element_chunks.append(chunks) num_chunks = len(chunk_set) r_sizes.append(num_chunks) if num_chunks == 0: continue weight = 1.0 / num_chunks for chunk in chunk_set: token_to_elems[chunk].append(i) token_value[chunk] = token_value.get(chunk, 0.0) + weight # value = sum of weights (for each chunk) # Step 2: Build heap (cost/value, token) heap = [] for chunk, val in token_value.items(): if val <= 0: continue try: cost = len(inverted_index.get_indexes(chunk)) # number of sets where chunk appears except ValueError: cost = float('inf') heapq.heappush(heap, (cost / val, chunk)) # Step 3: Greedy selection selected_sig = set() current_k_counts = [0] * n total_loss = float(n) while heap and total_loss >= theta: ratio, chunk = heapq.heappop(heap) if chunk in selected_sig: continue if ratio == float('inf'): break selected_sig.add(chunk) for i in range(n): if r_sizes[i] == 0: continue if chunk in element_chunks[i]: current_k_counts[i] += 1 total_loss = sum( r_sizes[i] / (r_sizes[i] + current_k_counts[i]) for i in range(n) if r_sizes[i] > 0 ) return list(selected_sig) __init__() Initialize the signature generator with default parameters. Source code in src/silkmoth/signature_generator.py def __init__(self): \"\"\" Initialize the signature generator with default parameters. \"\"\" self.sim_fun = jaccard_similarity self.q = 3 get_signature(reference_set, inverted_index, delta, alpha=0, sig_type=SigType.WEIGHTED, sim_fun=jaccard_similarity, q=3) Compute a signature for a reference set according to the chosen signature type. Parameters: Name Type Description Default reference_set list Tokenized reference set. required inverted_index InvertedIndex Index to evaluate token cost. required delta float Relatedness threshold factor. required alpha float Similarity threshold factor. 0 sig_type SigType Type of signature. WEIGHTED sim_fun callable Similarity function (phi) to use. jaccard_similarity q int Length of each q-chunk for edit similarity. 3 Returns: Name Type Description list list A list of str for selected tokens forming the signature. Source code in src/silkmoth/signature_generator.py def get_signature(self, reference_set, inverted_index, delta, alpha=0, sig_type=SigType.WEIGHTED, sim_fun = jaccard_similarity, q=3) -> list: \"\"\" Compute a signature for a reference set according to the chosen signature type. Args: reference_set (list): Tokenized reference set. inverted_index (InvertedIndex): Index to evaluate token cost. delta (float): Relatedness threshold factor. alpha (float): Similarity threshold factor. sig_type (SigType): Type of signature. sim_fun (callable): Similarity function (phi) to use. q (int): Length of each q-chunk for edit similarity. Returns: list: A list of str for selected tokens forming the signature. \"\"\" if sim_fun in (edit_similarity, N_edit_similarity): self.sim_fun = sim_fun self.q = q # If q is too large, no valid weighted signature exists q_bound = delta / (1 - delta) if q >= q_bound: warnings.warn( f\"q={q} is too large for delta = {delta:.3f}; \" \"no valid weighted signature exists -> falling back to brute-force.\" ) # returning all non-overlapping q-chunks so nothing is pruned all_chunks = [] for elem in reference_set: joined = \" \".join(elem) all_chunks += [ joined[i:i+q] for i in range(0, len(joined) - q + 1, q) ] return all_chunks is_edit_sim = sim_fun in (edit_similarity, N_edit_similarity) match sig_type: case SigType.WEIGHTED: if is_edit_sim and alpha > 0: return self._generate_simthresh_signature_edit_similarity(reference_set, inverted_index, delta, alpha) elif is_edit_sim and alpha == 0: return self._generate_weighted_signature_edit_similarity(reference_set, inverted_index, delta) else: return self._generate_weighted_signature(reference_set, inverted_index, delta) case SigType.SKYLINE: return self._generate_skyline_signature(reference_set, inverted_index, delta, alpha) case SigType.DICHOTOMY: return self._generate_dichotomy_signature(reference_set, inverted_index, delta, alpha) case _: raise ValueError(f\"Unknown signature type\")","title":"Signature Generator"},{"location":"pages/signature_generator/#silkmoth.signature_generator.SignatureGenerator","text":"The signature generator executes the signature generation step in the SilkMoth pipeline. During signature generation SilkMoth constructs a signature for a reference set R by selecting the \u201csmallest\u201d set of tokens from R such that if another set S does not share any tokens with R's signature, R and S are not related. SilkMoth supports three different signature schemes: Weighted Signature Scheme (Def. 5 in paper ), Skyline Signature Scheme (Def. 9), and Dichotomy Signature Scheme (Def. 10).","title":"SignatureGenerator"},{"location":"pages/signature_generator/#silkmoth.signature_generator.SignatureGenerator--examples","text":">>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.signature_generator import SignatureGenerator >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Sun\", \"Berlin\", \"Paris\"}] >>> sig_gen = SignatureGenerator() >>> sig_gen.get_signature(R, I, 0.3) ['Apple', 'Sun', 'Berlin'] >>> sig_gen.get_signature(R, I, 0.5) ['Apple', 'Berlin'] Source code in src/silkmoth/signature_generator.py class SignatureGenerator: \"\"\" The signature generator executes the signature generation step in the SilkMoth pipeline. During signature generation SilkMoth constructs a signature for a reference set R by selecting the \u201csmallest\u201d set of tokens from R such that if another set S does not share any tokens with R's signature, R and S are not related. SilkMoth supports three different signature schemes: Weighted Signature Scheme (Def. 5 in [paper](https://doi.org/10.14778/3115404.3115413)), Skyline Signature Scheme (Def. 9), and Dichotomy Signature Scheme (Def. 10). Examples -------- ``` >>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.signature_generator import SignatureGenerator >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Sun\", \"Berlin\", \"Paris\"}] >>> sig_gen = SignatureGenerator() >>> sig_gen.get_signature(R, I, 0.3) ['Apple', 'Sun', 'Berlin'] >>> sig_gen.get_signature(R, I, 0.5) ['Apple', 'Berlin'] ``` \"\"\" def __init__(self): \"\"\" Initialize the signature generator with default parameters. \"\"\" self.sim_fun = jaccard_similarity self.q = 3 def get_signature(self, reference_set, inverted_index, delta, alpha=0, sig_type=SigType.WEIGHTED, sim_fun = jaccard_similarity, q=3) -> list: \"\"\" Compute a signature for a reference set according to the chosen signature type. Args: reference_set (list): Tokenized reference set. inverted_index (InvertedIndex): Index to evaluate token cost. delta (float): Relatedness threshold factor. alpha (float): Similarity threshold factor. sig_type (SigType): Type of signature. sim_fun (callable): Similarity function (phi) to use. q (int): Length of each q-chunk for edit similarity. Returns: list: A list of str for selected tokens forming the signature. \"\"\" if sim_fun in (edit_similarity, N_edit_similarity): self.sim_fun = sim_fun self.q = q # If q is too large, no valid weighted signature exists q_bound = delta / (1 - delta) if q >= q_bound: warnings.warn( f\"q={q} is too large for delta = {delta:.3f}; \" \"no valid weighted signature exists -> falling back to brute-force.\" ) # returning all non-overlapping q-chunks so nothing is pruned all_chunks = [] for elem in reference_set: joined = \" \".join(elem) all_chunks += [ joined[i:i+q] for i in range(0, len(joined) - q + 1, q) ] return all_chunks is_edit_sim = sim_fun in (edit_similarity, N_edit_similarity) match sig_type: case SigType.WEIGHTED: if is_edit_sim and alpha > 0: return self._generate_simthresh_signature_edit_similarity(reference_set, inverted_index, delta, alpha) elif is_edit_sim and alpha == 0: return self._generate_weighted_signature_edit_similarity(reference_set, inverted_index, delta) else: return self._generate_weighted_signature(reference_set, inverted_index, delta) case SigType.SKYLINE: return self._generate_skyline_signature(reference_set, inverted_index, delta, alpha) case SigType.DICHOTOMY: return self._generate_dichotomy_signature(reference_set, inverted_index, delta, alpha) case _: raise ValueError(f\"Unknown signature type\") def _generate_simthresh_signature_edit_similarity(self, reference_set, inverted_index, delta, alpha) -> list: \"\"\" Builds a similarity-threshold signature for edit similarity as described in the SILKMOTH Paper in Section 7.2. For each element r_i in the reference set, it ensures that the signature contains at least m_i = floor((1 - alpha)/alpha * |q_chunks(r_i)|) + 1 q-chunks, so that any element sharing fewer than m_i chunks cannot achieve Eds \u2265 alpha. Args: reference_set (list): Tokenized reference set. inverted_index (InvertedIndex): Index to evaluate token cost. delta (float): Relatedness threshold factor. alpha (float): Similarity threshold factor. Returns: list: A list of str for selected q-chunks forming the signature. \"\"\" weighted_sig_edit_sim = set(self._generate_weighted_signature_edit_similarity(reference_set, inverted_index, delta)) simthresh_sig = set(weighted_sig_edit_sim) for elem_tokens in reference_set: # compute all q-chunks of each element joined = \" \".join(elem_tokens) chunks = [joined[j:j+self.q] for j in range(0, len(joined) - self.q + 1, self.q)] r = set(chunks) m_i = floor((1 - alpha) / alpha * len(r)) + 1 # determine how many base chunks from this element are already in weighted signature k_i = list(weighted_sig_edit_sim & r) if len(k_i) < m_i: # need cheapest additional chunks -> sort all chunks by cost = inverted_index size sorted_chunks = sorted( r, key=lambda t: len(inverted_index.get_indexes(t)) if t in inverted_index.lookup_table else float('inf') ) # add cheapest chunks up to m_i for chunk in sorted_chunks: if len(simthresh_sig & r) >= m_i: break simthresh_sig.add(chunk) return list(simthresh_sig) def _generate_skyline_signature(self, reference_set, inverted_index: InvertedIndex, delta, alpha): if self.sim_fun == jaccard_similarity: weighted = set(self._generate_weighted_signature(reference_set, inverted_index, delta)) unflattened = [weighted & set(r_i) for r_i in reference_set] elif self.sim_fun in (edit_similarity, N_edit_similarity): weighted = set(self._generate_weighted_signature_edit_similarity(reference_set, inverted_index, delta)) unflattened = [weighted & set(\" \".join(r_i)[i:i+self.q] for i in range(0, len(\" \".join(r_i)) - self.q + 1, self.q)) for r_i in reference_set] else: raise ValueError(f\"Unknown similarity function: {self.sim_fun}\") skyline = set() for i, k in enumerate(unflattened): if self.sim_fun == jaccard_similarity: rhs = floor((1 - alpha) * len(reference_set[i])) + 1 elif self.sim_fun in (edit_similarity, N_edit_similarity): chunks = get_q_chunks(reference_set[i], self.q) r = set(chunks) rhs = floor((1 - alpha) / alpha * len(r)) + 1 else: raise ValueError(f\"Unknown similarity function: {self.sim_fun}\") if len(k) < rhs: skyline |= k else: # add tokens with minimum |I[t]| tokens = list(k) tokens.sort(key=lambda t: len(inverted_index.get_indexes(t))) skyline = skyline.union(tokens[:rhs]) return list(skyline) def _generate_dichotomy_signature(self, reference_set, inverted_index: InvertedIndex, delta, alpha): \"\"\" Generates a signature using the Dichotomy Scheme as described in the SILKMOTH paper. For each element r_i, it chooses between its weighted signature part (k_i) and all its tokens (r_i) based on whether k_i is a subset of an optimal sim-thresh signature (m_i). \"\"\" # 1. First, generate the optimal weighted signature, K. if self.sim_fun == jaccard_similarity: weighted_signature_K = set(self._generate_weighted_signature(reference_set, inverted_index, delta)) elif self.sim_fun in (edit_similarity, N_edit_similarity): weighted_signature_K = set(self._generate_weighted_signature_edit_similarity(reference_set, inverted_index, delta)) else: raise ValueError(f\"Unknown similarity function: {self.sim_fun}\") final_dichotomy_sig = set() # 2. For each element r_i, decide whether to use its k_i or the full r_i. for r_i_list in reference_set: if self.sim_fun == jaccard_similarity: r_i = set(r_i_list) elif self.sim_fun in (edit_similarity, N_edit_similarity): chunks = get_q_chunks(r_i_list, self.q) r_i = set(chunks) else: raise ValueError(f\"Unknown similarity function: {self.sim_fun}\") if not r_i: continue # 3. Determine k_i: the part of the weighted signature in this element. k_i = weighted_signature_K.intersection(r_i) # 4. Determine m_i: the optimal sim-thresh signature for this element. # 4a. Calculate the required size for the sim-thresh signature. if self.sim_fun == jaccard_similarity: m_i_size = floor((1 - alpha) * len(r_i)) + 1 elif self.sim_fun in (edit_similarity, N_edit_similarity): m_i_size = floor((1 - alpha) / alpha * len(r_i)) + 1 else: raise ValueError(f\"Unknown similarity function: {self.sim_fun}\") # 4b. Get all tokens from the original element r_i and sort by cost. element_tokens = list(r_i) # Sort tokens by the length of their inverted index list (cost). # Handle cases where a token might not be in the index. def get_token_cost(token): try: return len(inverted_index.get_indexes(token)) except ValueError: return float('inf') # Assign a high cost if not found element_tokens.sort(key=get_token_cost) # 4c. The optimal m_i consists of the cheapest tokens. m_i = set(element_tokens[:m_i_size]) # 5. The Decision: Apply the paper's condition. # Can we get away with the cheaper weighted signature part (k_i)? # Yes, if k_i is already a subset of the optimal sim-thresh signature (m_i). if k_i.issubset(m_i): # Decision: Use the cheaper weighted signature tokens. final_dichotomy_sig.update(k_i) else: # Decision: Fall back to the safe, more expensive option. final_dichotomy_sig.update(r_i) return list(final_dichotomy_sig) def _generate_weighted_signature(self, reference_set, inverted_index, delta): if delta <= 0.0: return [] n = len(reference_set) theta = delta * n # required covered fraction , delta * |R| in paper # 1) Build token: elements map and aggregate token values token_to_elems = defaultdict(list) token_value = {} for i, elem in enumerate(reference_set): if not elem: warnings.warn(f\"Element at index {i} is empty and will be skipped.\") continue unique_tokens = set(elem) # remove duplicate tokens inside each element weight = 1.0 / len(unique_tokens) for t in unique_tokens: token_to_elems[t].append(i) token_value[t] = token_value.get(t, 0.0) + weight # value = sum of weights (for each token) # 2) Build min-heap of (cost/value, token) heap = [] for t, val in token_value.items(): if val <= 0: continue try: cost = len(inverted_index.get_indexes(t)) # look up each token in inverted index to count in how many sets it is = cost except ValueError: # Token not in index: assign infinite cost to deprioritize cost = float('inf') heapq.heappush(heap, (cost / val, t)) # goal small ratio: cost/value # 3) Selection with greedy algorithm selected_sig = set() r_sizes = [len(set(elem)) if elem else 0 for elem in reference_set] total_loss = float(n) current_k_counts = [0] * n # while heap and total_loss >= theta: while heap and total_loss >= theta: # 1. ratio, t = heapq.heappop(heap) # pull best token with lowest cost/value from heap if t in selected_sig: continue if ratio == float('inf'): break # 2. selected_sig.add(t) # 3. for i in range(n): if r_sizes[i] == 0: continue # Calculate |k_i|: number of tokens from reference_set[i] also in selected_sig current_k_counts[i] = len(set(reference_set[i]).intersection(selected_sig)) # 4. total_loss = sum( (r_sizes[i] - current_k_counts[i]) / r_sizes[i] for i in range(n) if r_sizes[i] > 0 ) return list(selected_sig) # Following the same logic of _generate_weighted_signature def _generate_weighted_signature_edit_similarity(self, reference_set, inverted_index, delta): if delta <= 0.0: return [] n = len(reference_set) theta = delta * n token_to_elems = defaultdict(list) # map q-chunk -> list of element indexes token_value = {} # map q-chunk -> accumulated value r_sizes = [] # number of q-chunks per element element_chunks = [] # list of q-chunks per element # Step 1: Build q-chunks and token values for i, elem_tokens in enumerate(reference_set): if not elem_tokens: # warnings.warn(f\"Element at index {i} is empty and will be skipped.\") r_sizes.append(0) element_chunks.append([]) continue # Join the list of tokens into a string joined = \" \".join(elem_tokens) # Extract non-overlapping q-chunks chunks = [joined[j:j+self.q] for j in range(0, len(joined) - self.q + 1, self.q)] chunk_set = set(chunks) element_chunks.append(chunks) num_chunks = len(chunk_set) r_sizes.append(num_chunks) if num_chunks == 0: continue weight = 1.0 / num_chunks for chunk in chunk_set: token_to_elems[chunk].append(i) token_value[chunk] = token_value.get(chunk, 0.0) + weight # value = sum of weights (for each chunk) # Step 2: Build heap (cost/value, token) heap = [] for chunk, val in token_value.items(): if val <= 0: continue try: cost = len(inverted_index.get_indexes(chunk)) # number of sets where chunk appears except ValueError: cost = float('inf') heapq.heappush(heap, (cost / val, chunk)) # Step 3: Greedy selection selected_sig = set() current_k_counts = [0] * n total_loss = float(n) while heap and total_loss >= theta: ratio, chunk = heapq.heappop(heap) if chunk in selected_sig: continue if ratio == float('inf'): break selected_sig.add(chunk) for i in range(n): if r_sizes[i] == 0: continue if chunk in element_chunks[i]: current_k_counts[i] += 1 total_loss = sum( r_sizes[i] / (r_sizes[i] + current_k_counts[i]) for i in range(n) if r_sizes[i] > 0 ) return list(selected_sig)","title":"Examples"},{"location":"pages/signature_generator/#silkmoth.signature_generator.SignatureGenerator.__init__","text":"Initialize the signature generator with default parameters. Source code in src/silkmoth/signature_generator.py def __init__(self): \"\"\" Initialize the signature generator with default parameters. \"\"\" self.sim_fun = jaccard_similarity self.q = 3","title":"__init__"},{"location":"pages/signature_generator/#silkmoth.signature_generator.SignatureGenerator.get_signature","text":"Compute a signature for a reference set according to the chosen signature type. Parameters: Name Type Description Default reference_set list Tokenized reference set. required inverted_index InvertedIndex Index to evaluate token cost. required delta float Relatedness threshold factor. required alpha float Similarity threshold factor. 0 sig_type SigType Type of signature. WEIGHTED sim_fun callable Similarity function (phi) to use. jaccard_similarity q int Length of each q-chunk for edit similarity. 3 Returns: Name Type Description list list A list of str for selected tokens forming the signature. Source code in src/silkmoth/signature_generator.py def get_signature(self, reference_set, inverted_index, delta, alpha=0, sig_type=SigType.WEIGHTED, sim_fun = jaccard_similarity, q=3) -> list: \"\"\" Compute a signature for a reference set according to the chosen signature type. Args: reference_set (list): Tokenized reference set. inverted_index (InvertedIndex): Index to evaluate token cost. delta (float): Relatedness threshold factor. alpha (float): Similarity threshold factor. sig_type (SigType): Type of signature. sim_fun (callable): Similarity function (phi) to use. q (int): Length of each q-chunk for edit similarity. Returns: list: A list of str for selected tokens forming the signature. \"\"\" if sim_fun in (edit_similarity, N_edit_similarity): self.sim_fun = sim_fun self.q = q # If q is too large, no valid weighted signature exists q_bound = delta / (1 - delta) if q >= q_bound: warnings.warn( f\"q={q} is too large for delta = {delta:.3f}; \" \"no valid weighted signature exists -> falling back to brute-force.\" ) # returning all non-overlapping q-chunks so nothing is pruned all_chunks = [] for elem in reference_set: joined = \" \".join(elem) all_chunks += [ joined[i:i+q] for i in range(0, len(joined) - q + 1, q) ] return all_chunks is_edit_sim = sim_fun in (edit_similarity, N_edit_similarity) match sig_type: case SigType.WEIGHTED: if is_edit_sim and alpha > 0: return self._generate_simthresh_signature_edit_similarity(reference_set, inverted_index, delta, alpha) elif is_edit_sim and alpha == 0: return self._generate_weighted_signature_edit_similarity(reference_set, inverted_index, delta) else: return self._generate_weighted_signature(reference_set, inverted_index, delta) case SigType.SKYLINE: return self._generate_skyline_signature(reference_set, inverted_index, delta, alpha) case SigType.DICHOTOMY: return self._generate_dichotomy_signature(reference_set, inverted_index, delta, alpha) case _: raise ValueError(f\"Unknown signature type\")","title":"get_signature"},{"location":"pages/silkmoth_engine/","text":"SilkMothEngine The SilkMothEngine is the system's main component. It brings all the SilkMoth components together, enabling users to easily vary the system's setup and explore the relationships between their input sets. Examples >>> from silkmoth.silkmoth_engine import SilkMothEngine >>> from silkmoth.utils import contain >>> S = [ ... ['Mass Ave St Boston 02115', '77 Mass 5th St Boston', '77 Mass Ave 5th 02115'], ... ['77 Boston MA', '77 5th St Boston 02115', '77 Mass Ave 02115 Seattle'], ... ['77 Mass Ave 5th Boston MA', 'Mass Ave Chicago IL', '77 Mass Ave St'], ... ['77 Mass Ave MA', '5th St 02115 Seattle WA', '77 5th St Boston Seattle'] ... ] >>> R = ['77 Mass Ave Boston MA', '5th St 02115 Seattle WA', '77 5th St Chicago IL'] >>> engine = SilkMothEngine(0.7, S, contain) >>> results, _, _ = engine.search_sets(R) >>> results [(3, 0.7428571428571429)] >>> engine.set_related_threshold(0.3) >>> results, _, _ = engine.search_sets(R) >>> results [(0, 0.36904761904761907), (1, 0.4261904761904762), (2, 0.4146825396825397), (3, 0.7428571428571429)] Source code in src/silkmoth/silkmoth_engine.py class SilkMothEngine: \"\"\" The SilkMothEngine is the system's main component. It brings all the SilkMoth components together, enabling users to easily vary the system's setup and explore the relationships between their input sets. Examples -------- ``` >>> from silkmoth.silkmoth_engine import SilkMothEngine >>> from silkmoth.utils import contain >>> S = [ ... ['Mass Ave St Boston 02115', '77 Mass 5th St Boston', '77 Mass Ave 5th 02115'], ... ['77 Boston MA', '77 5th St Boston 02115', '77 Mass Ave 02115 Seattle'], ... ['77 Mass Ave 5th Boston MA', 'Mass Ave Chicago IL', '77 Mass Ave St'], ... ['77 Mass Ave MA', '5th St 02115 Seattle WA', '77 5th St Boston Seattle'] ... ] >>> R = ['77 Mass Ave Boston MA', '5th St 02115 Seattle WA', '77 5th St Chicago IL'] >>> engine = SilkMothEngine(0.7, S, contain) >>> results, _, _ = engine.search_sets(R) >>> results [(3, 0.7428571428571429)] >>> engine.set_related_threshold(0.3) >>> results, _, _ = engine.search_sets(R) >>> results [(0, 0.36904761904761907), (1, 0.4261904761904762), (2, 0.4146825396825397), (3, 0.7428571428571429)] ``` \"\"\" def __init__(self, related_thresh, source_sets, sim_metric=similar, sim_func=jaccard_similarity, sim_thresh=0, reduction=False, sig_type=SigType.WEIGHTED, is_check_filter=False, is_nn_filter=False, q=3): \"\"\" Initialize the SilkMothEngine with all the necessary parameters. Args: related_thresh (float): Relatedness threshold delta source_sets (list): Collection of source sets sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction sig_type (SigType): Type of signature. is_check_filter (bool): Flag to activate/deactivate check filter is_nn_filter (bool): Flag to activate/deactivate nearest neighbor filter q (int): The q-gram size for tokenization \"\"\" self.related_thresh = related_thresh # delta self.source_sets = source_sets # S self.sim_metric = sim_metric # related self.sim_func = sim_func # phi self.sim_thresh = sim_thresh # alpha self.q = q # q-gram size self.reduction = reduction self.signature_type = sig_type self.tokenizer = Tokenizer(sim_func, q) self.is_check_filter = is_check_filter self.is_nn_filter = is_nn_filter self.signature_gen = SignatureGenerator() self.candidate_selector = self._create_candidate_selector() self.verifier = self._create_verifier() self.inverted_index = self.build_index(source_sets) def build_index(self, source_sets) -> InvertedIndex: \"\"\" Tokenizes all source sets and creates the inverted index. Args: source_sets (list): Collection of \"raw\" source sets Returns: InvertedIndex: Inverted index \"\"\" token_sets = [self.tokenizer.tokenize(s) for s in source_sets] return InvertedIndex(token_sets) def search_sets(self, reference_set) -> tuple[list, int, int]: \"\"\" Search mode, where, given a reference set, we search for all related sets in the dataset. Args: reference_set (list): \"Raw\" reference set Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. int: Number of candidates before applying filters. int: Number of candidates after applying filters. \"\"\" r_tokens = self.tokenizer.tokenize(reference_set) signature = self.signature_gen.get_signature(r_tokens, self.inverted_index, self.related_thresh, self.sim_thresh, self.signature_type, self.sim_func, self.q) candidates = self.candidate_selector.get_candidates(signature, self.inverted_index, len(r_tokens)) # Count how many candidates are removed by the filters candidates_start = len(candidates) # Apply check filter if enabled if self.is_check_filter: candidates, match_map = self.candidate_selector.check_filter( r_tokens, set(signature), candidates, self.inverted_index ) else: match_map = None # Apply nearest neighbor filter if enabled if self.is_nn_filter: candidates= self.candidate_selector.nn_filter( r_tokens, set(signature), candidates, self.inverted_index, self.related_thresh, match_map ) return self.verifier.get_related_sets(r_tokens, candidates, self.inverted_index), candidates_start , len(candidates) def discover_sets(self, reference_sets) -> list: \"\"\" Discovery mode, where we search for all pairs of related sets within a collection of reference sets. Args: reference_sets (list): Collection of \"raw\" reference set Returns: list: Tuples (i, j, sim) of all related sets with reference index i, source set index j and the computed similarity score sim. \"\"\" #related_pairs = [] for i, reference_set in enumerate(reference_sets): sets, _, _ = self.search_sets(reference_set) #related_pairs.extend([(i, j, sim) for j, sim in sets]) #return related_pairs def set_related_threshold(self, related_thresh): \"\"\" Updates the relatedness threshold. Args: related_thresh (float): Relatedness threshold delta \"\"\" self.related_thresh = related_thresh self.verifier = self._create_verifier() self.candidate_selector = self._create_candidate_selector() def set_signature_type(self, sig_type): \"\"\" Updates the signature type. Args: sig_type (SigType): Signature type \"\"\" self.signature_type = sig_type def set_check_filter(self, is_check_filter): \"\"\" Updates the check filter flag. Args: is_check_filter (bool): Flag to activate/deactivate check filter \"\"\" self.is_check_filter = is_check_filter def set_nn_filter(self, is_nn_filter): \"\"\" Updates the nearest neighbor filter flag. Args: is_nn_filter (bool): Flag to activate/deactivate nearest neighbor filter \"\"\" self.is_nn_filter = is_nn_filter def set_alpha(self, sim_thresh): \"\"\" Updates the similarity threshold. Args: sim_thresh (float): Similarity threshold alpha \"\"\" self.sim_thresh = sim_thresh self.verifier = self._create_verifier() self.candidate_selector = self._create_candidate_selector() def set_reduction(self, reduction): \"\"\" Updates the reduction flag. Args: reduction (bool): Flag to activate/deactivate reduction \"\"\" self.reduction = reduction self.verifier = self._create_verifier() def _create_verifier(self): if self.reduction and self.sim_thresh > 0: self.reduction = False warnings.warn(\"\"\"\"Reduction-based verification does not work when the similarity threshold alpha is greater than zero. Reduction is disabled for now.\"\"\") return Verifier( self.related_thresh, self.sim_metric, self.sim_func, self.sim_thresh, self.reduction ) def _create_candidate_selector(self): return CandidateSelector( self.sim_func, self.sim_metric, self.related_thresh, self.sim_thresh ) def set_q(self, q): \"\"\" Updates q-gram size. Args: q (int): The q-gram size for tokenization \"\"\" self.q = q self.tokenizer = Tokenizer(self.sim_func, q) self.inverted_index = self.build_index(self.source_sets) self.signature_gen = SignatureGenerator() self.candidate_selector = self._create_candidate_selector() self.verifier = self._create_verifier() __init__(related_thresh, source_sets, sim_metric=similar, sim_func=jaccard_similarity, sim_thresh=0, reduction=False, sig_type=SigType.WEIGHTED, is_check_filter=False, is_nn_filter=False, q=3) Initialize the SilkMothEngine with all the necessary parameters. Parameters: Name Type Description Default related_thresh float Relatedness threshold delta required source_sets list Collection of source sets required sim_metric callable Similarity metric similar(...)/contain(...) similar sim_func callable Similarity function phi jaccard_similarity sim_thresh float Similarity threshold alpha 0 reduction bool Flag to activate/deactivate triangle inequality reduction False sig_type SigType Type of signature. WEIGHTED is_check_filter bool Flag to activate/deactivate check filter False is_nn_filter bool Flag to activate/deactivate nearest neighbor filter False q int The q-gram size for tokenization 3 Source code in src/silkmoth/silkmoth_engine.py def __init__(self, related_thresh, source_sets, sim_metric=similar, sim_func=jaccard_similarity, sim_thresh=0, reduction=False, sig_type=SigType.WEIGHTED, is_check_filter=False, is_nn_filter=False, q=3): \"\"\" Initialize the SilkMothEngine with all the necessary parameters. Args: related_thresh (float): Relatedness threshold delta source_sets (list): Collection of source sets sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction sig_type (SigType): Type of signature. is_check_filter (bool): Flag to activate/deactivate check filter is_nn_filter (bool): Flag to activate/deactivate nearest neighbor filter q (int): The q-gram size for tokenization \"\"\" self.related_thresh = related_thresh # delta self.source_sets = source_sets # S self.sim_metric = sim_metric # related self.sim_func = sim_func # phi self.sim_thresh = sim_thresh # alpha self.q = q # q-gram size self.reduction = reduction self.signature_type = sig_type self.tokenizer = Tokenizer(sim_func, q) self.is_check_filter = is_check_filter self.is_nn_filter = is_nn_filter self.signature_gen = SignatureGenerator() self.candidate_selector = self._create_candidate_selector() self.verifier = self._create_verifier() self.inverted_index = self.build_index(source_sets) build_index(source_sets) Tokenizes all source sets and creates the inverted index. Parameters: Name Type Description Default source_sets list Collection of \"raw\" source sets required Returns: Name Type Description InvertedIndex InvertedIndex Inverted index Source code in src/silkmoth/silkmoth_engine.py def build_index(self, source_sets) -> InvertedIndex: \"\"\" Tokenizes all source sets and creates the inverted index. Args: source_sets (list): Collection of \"raw\" source sets Returns: InvertedIndex: Inverted index \"\"\" token_sets = [self.tokenizer.tokenize(s) for s in source_sets] return InvertedIndex(token_sets) discover_sets(reference_sets) Discovery mode, where we search for all pairs of related sets within a collection of reference sets. Parameters: Name Type Description Default reference_sets list Collection of \"raw\" reference set required Returns: Name Type Description list list Tuples (i, j, sim) of all related sets with reference index i, source set index j and the computed similarity score sim. Source code in src/silkmoth/silkmoth_engine.py def discover_sets(self, reference_sets) -> list: \"\"\" Discovery mode, where we search for all pairs of related sets within a collection of reference sets. Args: reference_sets (list): Collection of \"raw\" reference set Returns: list: Tuples (i, j, sim) of all related sets with reference index i, source set index j and the computed similarity score sim. \"\"\" #related_pairs = [] for i, reference_set in enumerate(reference_sets): sets, _, _ = self.search_sets(reference_set) search_sets(reference_set) Search mode, where, given a reference set, we search for all related sets in the dataset. Parameters: Name Type Description Default reference_set list \"Raw\" reference set required Returns: Name Type Description list list Pairs of indices of all related sets from the candidates and their relatedness with the reference set. int int Number of candidates before applying filters. int int Number of candidates after applying filters. Source code in src/silkmoth/silkmoth_engine.py def search_sets(self, reference_set) -> tuple[list, int, int]: \"\"\" Search mode, where, given a reference set, we search for all related sets in the dataset. Args: reference_set (list): \"Raw\" reference set Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. int: Number of candidates before applying filters. int: Number of candidates after applying filters. \"\"\" r_tokens = self.tokenizer.tokenize(reference_set) signature = self.signature_gen.get_signature(r_tokens, self.inverted_index, self.related_thresh, self.sim_thresh, self.signature_type, self.sim_func, self.q) candidates = self.candidate_selector.get_candidates(signature, self.inverted_index, len(r_tokens)) # Count how many candidates are removed by the filters candidates_start = len(candidates) # Apply check filter if enabled if self.is_check_filter: candidates, match_map = self.candidate_selector.check_filter( r_tokens, set(signature), candidates, self.inverted_index ) else: match_map = None # Apply nearest neighbor filter if enabled if self.is_nn_filter: candidates= self.candidate_selector.nn_filter( r_tokens, set(signature), candidates, self.inverted_index, self.related_thresh, match_map ) return self.verifier.get_related_sets(r_tokens, candidates, self.inverted_index), candidates_start , len(candidates) set_alpha(sim_thresh) Updates the similarity threshold. Parameters: Name Type Description Default sim_thresh float Similarity threshold alpha required Source code in src/silkmoth/silkmoth_engine.py def set_alpha(self, sim_thresh): \"\"\" Updates the similarity threshold. Args: sim_thresh (float): Similarity threshold alpha \"\"\" self.sim_thresh = sim_thresh self.verifier = self._create_verifier() self.candidate_selector = self._create_candidate_selector() set_check_filter(is_check_filter) Updates the check filter flag. Parameters: Name Type Description Default is_check_filter bool Flag to activate/deactivate check filter required Source code in src/silkmoth/silkmoth_engine.py def set_check_filter(self, is_check_filter): \"\"\" Updates the check filter flag. Args: is_check_filter (bool): Flag to activate/deactivate check filter \"\"\" self.is_check_filter = is_check_filter set_nn_filter(is_nn_filter) Updates the nearest neighbor filter flag. Parameters: Name Type Description Default is_nn_filter bool Flag to activate/deactivate nearest neighbor filter required Source code in src/silkmoth/silkmoth_engine.py def set_nn_filter(self, is_nn_filter): \"\"\" Updates the nearest neighbor filter flag. Args: is_nn_filter (bool): Flag to activate/deactivate nearest neighbor filter \"\"\" self.is_nn_filter = is_nn_filter set_q(q) Updates q-gram size. Parameters: Name Type Description Default q int The q-gram size for tokenization required Source code in src/silkmoth/silkmoth_engine.py def set_q(self, q): \"\"\" Updates q-gram size. Args: q (int): The q-gram size for tokenization \"\"\" self.q = q self.tokenizer = Tokenizer(self.sim_func, q) self.inverted_index = self.build_index(self.source_sets) self.signature_gen = SignatureGenerator() self.candidate_selector = self._create_candidate_selector() self.verifier = self._create_verifier() set_reduction(reduction) Updates the reduction flag. Parameters: Name Type Description Default reduction bool Flag to activate/deactivate reduction required Source code in src/silkmoth/silkmoth_engine.py def set_reduction(self, reduction): \"\"\" Updates the reduction flag. Args: reduction (bool): Flag to activate/deactivate reduction \"\"\" self.reduction = reduction self.verifier = self._create_verifier() set_related_threshold(related_thresh) Updates the relatedness threshold. Parameters: Name Type Description Default related_thresh float Relatedness threshold delta required Source code in src/silkmoth/silkmoth_engine.py def set_related_threshold(self, related_thresh): \"\"\" Updates the relatedness threshold. Args: related_thresh (float): Relatedness threshold delta \"\"\" self.related_thresh = related_thresh self.verifier = self._create_verifier() self.candidate_selector = self._create_candidate_selector() set_signature_type(sig_type) Updates the signature type. Parameters: Name Type Description Default sig_type SigType Signature type required Source code in src/silkmoth/silkmoth_engine.py def set_signature_type(self, sig_type): \"\"\" Updates the signature type. Args: sig_type (SigType): Signature type \"\"\" self.signature_type = sig_type","title":"Engine"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine","text":"The SilkMothEngine is the system's main component. It brings all the SilkMoth components together, enabling users to easily vary the system's setup and explore the relationships between their input sets.","title":"SilkMothEngine"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine--examples","text":">>> from silkmoth.silkmoth_engine import SilkMothEngine >>> from silkmoth.utils import contain >>> S = [ ... ['Mass Ave St Boston 02115', '77 Mass 5th St Boston', '77 Mass Ave 5th 02115'], ... ['77 Boston MA', '77 5th St Boston 02115', '77 Mass Ave 02115 Seattle'], ... ['77 Mass Ave 5th Boston MA', 'Mass Ave Chicago IL', '77 Mass Ave St'], ... ['77 Mass Ave MA', '5th St 02115 Seattle WA', '77 5th St Boston Seattle'] ... ] >>> R = ['77 Mass Ave Boston MA', '5th St 02115 Seattle WA', '77 5th St Chicago IL'] >>> engine = SilkMothEngine(0.7, S, contain) >>> results, _, _ = engine.search_sets(R) >>> results [(3, 0.7428571428571429)] >>> engine.set_related_threshold(0.3) >>> results, _, _ = engine.search_sets(R) >>> results [(0, 0.36904761904761907), (1, 0.4261904761904762), (2, 0.4146825396825397), (3, 0.7428571428571429)] Source code in src/silkmoth/silkmoth_engine.py class SilkMothEngine: \"\"\" The SilkMothEngine is the system's main component. It brings all the SilkMoth components together, enabling users to easily vary the system's setup and explore the relationships between their input sets. Examples -------- ``` >>> from silkmoth.silkmoth_engine import SilkMothEngine >>> from silkmoth.utils import contain >>> S = [ ... ['Mass Ave St Boston 02115', '77 Mass 5th St Boston', '77 Mass Ave 5th 02115'], ... ['77 Boston MA', '77 5th St Boston 02115', '77 Mass Ave 02115 Seattle'], ... ['77 Mass Ave 5th Boston MA', 'Mass Ave Chicago IL', '77 Mass Ave St'], ... ['77 Mass Ave MA', '5th St 02115 Seattle WA', '77 5th St Boston Seattle'] ... ] >>> R = ['77 Mass Ave Boston MA', '5th St 02115 Seattle WA', '77 5th St Chicago IL'] >>> engine = SilkMothEngine(0.7, S, contain) >>> results, _, _ = engine.search_sets(R) >>> results [(3, 0.7428571428571429)] >>> engine.set_related_threshold(0.3) >>> results, _, _ = engine.search_sets(R) >>> results [(0, 0.36904761904761907), (1, 0.4261904761904762), (2, 0.4146825396825397), (3, 0.7428571428571429)] ``` \"\"\" def __init__(self, related_thresh, source_sets, sim_metric=similar, sim_func=jaccard_similarity, sim_thresh=0, reduction=False, sig_type=SigType.WEIGHTED, is_check_filter=False, is_nn_filter=False, q=3): \"\"\" Initialize the SilkMothEngine with all the necessary parameters. Args: related_thresh (float): Relatedness threshold delta source_sets (list): Collection of source sets sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction sig_type (SigType): Type of signature. is_check_filter (bool): Flag to activate/deactivate check filter is_nn_filter (bool): Flag to activate/deactivate nearest neighbor filter q (int): The q-gram size for tokenization \"\"\" self.related_thresh = related_thresh # delta self.source_sets = source_sets # S self.sim_metric = sim_metric # related self.sim_func = sim_func # phi self.sim_thresh = sim_thresh # alpha self.q = q # q-gram size self.reduction = reduction self.signature_type = sig_type self.tokenizer = Tokenizer(sim_func, q) self.is_check_filter = is_check_filter self.is_nn_filter = is_nn_filter self.signature_gen = SignatureGenerator() self.candidate_selector = self._create_candidate_selector() self.verifier = self._create_verifier() self.inverted_index = self.build_index(source_sets) def build_index(self, source_sets) -> InvertedIndex: \"\"\" Tokenizes all source sets and creates the inverted index. Args: source_sets (list): Collection of \"raw\" source sets Returns: InvertedIndex: Inverted index \"\"\" token_sets = [self.tokenizer.tokenize(s) for s in source_sets] return InvertedIndex(token_sets) def search_sets(self, reference_set) -> tuple[list, int, int]: \"\"\" Search mode, where, given a reference set, we search for all related sets in the dataset. Args: reference_set (list): \"Raw\" reference set Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. int: Number of candidates before applying filters. int: Number of candidates after applying filters. \"\"\" r_tokens = self.tokenizer.tokenize(reference_set) signature = self.signature_gen.get_signature(r_tokens, self.inverted_index, self.related_thresh, self.sim_thresh, self.signature_type, self.sim_func, self.q) candidates = self.candidate_selector.get_candidates(signature, self.inverted_index, len(r_tokens)) # Count how many candidates are removed by the filters candidates_start = len(candidates) # Apply check filter if enabled if self.is_check_filter: candidates, match_map = self.candidate_selector.check_filter( r_tokens, set(signature), candidates, self.inverted_index ) else: match_map = None # Apply nearest neighbor filter if enabled if self.is_nn_filter: candidates= self.candidate_selector.nn_filter( r_tokens, set(signature), candidates, self.inverted_index, self.related_thresh, match_map ) return self.verifier.get_related_sets(r_tokens, candidates, self.inverted_index), candidates_start , len(candidates) def discover_sets(self, reference_sets) -> list: \"\"\" Discovery mode, where we search for all pairs of related sets within a collection of reference sets. Args: reference_sets (list): Collection of \"raw\" reference set Returns: list: Tuples (i, j, sim) of all related sets with reference index i, source set index j and the computed similarity score sim. \"\"\" #related_pairs = [] for i, reference_set in enumerate(reference_sets): sets, _, _ = self.search_sets(reference_set) #related_pairs.extend([(i, j, sim) for j, sim in sets]) #return related_pairs def set_related_threshold(self, related_thresh): \"\"\" Updates the relatedness threshold. Args: related_thresh (float): Relatedness threshold delta \"\"\" self.related_thresh = related_thresh self.verifier = self._create_verifier() self.candidate_selector = self._create_candidate_selector() def set_signature_type(self, sig_type): \"\"\" Updates the signature type. Args: sig_type (SigType): Signature type \"\"\" self.signature_type = sig_type def set_check_filter(self, is_check_filter): \"\"\" Updates the check filter flag. Args: is_check_filter (bool): Flag to activate/deactivate check filter \"\"\" self.is_check_filter = is_check_filter def set_nn_filter(self, is_nn_filter): \"\"\" Updates the nearest neighbor filter flag. Args: is_nn_filter (bool): Flag to activate/deactivate nearest neighbor filter \"\"\" self.is_nn_filter = is_nn_filter def set_alpha(self, sim_thresh): \"\"\" Updates the similarity threshold. Args: sim_thresh (float): Similarity threshold alpha \"\"\" self.sim_thresh = sim_thresh self.verifier = self._create_verifier() self.candidate_selector = self._create_candidate_selector() def set_reduction(self, reduction): \"\"\" Updates the reduction flag. Args: reduction (bool): Flag to activate/deactivate reduction \"\"\" self.reduction = reduction self.verifier = self._create_verifier() def _create_verifier(self): if self.reduction and self.sim_thresh > 0: self.reduction = False warnings.warn(\"\"\"\"Reduction-based verification does not work when the similarity threshold alpha is greater than zero. Reduction is disabled for now.\"\"\") return Verifier( self.related_thresh, self.sim_metric, self.sim_func, self.sim_thresh, self.reduction ) def _create_candidate_selector(self): return CandidateSelector( self.sim_func, self.sim_metric, self.related_thresh, self.sim_thresh ) def set_q(self, q): \"\"\" Updates q-gram size. Args: q (int): The q-gram size for tokenization \"\"\" self.q = q self.tokenizer = Tokenizer(self.sim_func, q) self.inverted_index = self.build_index(self.source_sets) self.signature_gen = SignatureGenerator() self.candidate_selector = self._create_candidate_selector() self.verifier = self._create_verifier()","title":"Examples"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.__init__","text":"Initialize the SilkMothEngine with all the necessary parameters. Parameters: Name Type Description Default related_thresh float Relatedness threshold delta required source_sets list Collection of source sets required sim_metric callable Similarity metric similar(...)/contain(...) similar sim_func callable Similarity function phi jaccard_similarity sim_thresh float Similarity threshold alpha 0 reduction bool Flag to activate/deactivate triangle inequality reduction False sig_type SigType Type of signature. WEIGHTED is_check_filter bool Flag to activate/deactivate check filter False is_nn_filter bool Flag to activate/deactivate nearest neighbor filter False q int The q-gram size for tokenization 3 Source code in src/silkmoth/silkmoth_engine.py def __init__(self, related_thresh, source_sets, sim_metric=similar, sim_func=jaccard_similarity, sim_thresh=0, reduction=False, sig_type=SigType.WEIGHTED, is_check_filter=False, is_nn_filter=False, q=3): \"\"\" Initialize the SilkMothEngine with all the necessary parameters. Args: related_thresh (float): Relatedness threshold delta source_sets (list): Collection of source sets sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction sig_type (SigType): Type of signature. is_check_filter (bool): Flag to activate/deactivate check filter is_nn_filter (bool): Flag to activate/deactivate nearest neighbor filter q (int): The q-gram size for tokenization \"\"\" self.related_thresh = related_thresh # delta self.source_sets = source_sets # S self.sim_metric = sim_metric # related self.sim_func = sim_func # phi self.sim_thresh = sim_thresh # alpha self.q = q # q-gram size self.reduction = reduction self.signature_type = sig_type self.tokenizer = Tokenizer(sim_func, q) self.is_check_filter = is_check_filter self.is_nn_filter = is_nn_filter self.signature_gen = SignatureGenerator() self.candidate_selector = self._create_candidate_selector() self.verifier = self._create_verifier() self.inverted_index = self.build_index(source_sets)","title":"__init__"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.build_index","text":"Tokenizes all source sets and creates the inverted index. Parameters: Name Type Description Default source_sets list Collection of \"raw\" source sets required Returns: Name Type Description InvertedIndex InvertedIndex Inverted index Source code in src/silkmoth/silkmoth_engine.py def build_index(self, source_sets) -> InvertedIndex: \"\"\" Tokenizes all source sets and creates the inverted index. Args: source_sets (list): Collection of \"raw\" source sets Returns: InvertedIndex: Inverted index \"\"\" token_sets = [self.tokenizer.tokenize(s) for s in source_sets] return InvertedIndex(token_sets)","title":"build_index"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.discover_sets","text":"Discovery mode, where we search for all pairs of related sets within a collection of reference sets. Parameters: Name Type Description Default reference_sets list Collection of \"raw\" reference set required Returns: Name Type Description list list Tuples (i, j, sim) of all related sets with reference index i, source set index j and the computed similarity score sim. Source code in src/silkmoth/silkmoth_engine.py def discover_sets(self, reference_sets) -> list: \"\"\" Discovery mode, where we search for all pairs of related sets within a collection of reference sets. Args: reference_sets (list): Collection of \"raw\" reference set Returns: list: Tuples (i, j, sim) of all related sets with reference index i, source set index j and the computed similarity score sim. \"\"\" #related_pairs = [] for i, reference_set in enumerate(reference_sets): sets, _, _ = self.search_sets(reference_set)","title":"discover_sets"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.search_sets","text":"Search mode, where, given a reference set, we search for all related sets in the dataset. Parameters: Name Type Description Default reference_set list \"Raw\" reference set required Returns: Name Type Description list list Pairs of indices of all related sets from the candidates and their relatedness with the reference set. int int Number of candidates before applying filters. int int Number of candidates after applying filters. Source code in src/silkmoth/silkmoth_engine.py def search_sets(self, reference_set) -> tuple[list, int, int]: \"\"\" Search mode, where, given a reference set, we search for all related sets in the dataset. Args: reference_set (list): \"Raw\" reference set Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. int: Number of candidates before applying filters. int: Number of candidates after applying filters. \"\"\" r_tokens = self.tokenizer.tokenize(reference_set) signature = self.signature_gen.get_signature(r_tokens, self.inverted_index, self.related_thresh, self.sim_thresh, self.signature_type, self.sim_func, self.q) candidates = self.candidate_selector.get_candidates(signature, self.inverted_index, len(r_tokens)) # Count how many candidates are removed by the filters candidates_start = len(candidates) # Apply check filter if enabled if self.is_check_filter: candidates, match_map = self.candidate_selector.check_filter( r_tokens, set(signature), candidates, self.inverted_index ) else: match_map = None # Apply nearest neighbor filter if enabled if self.is_nn_filter: candidates= self.candidate_selector.nn_filter( r_tokens, set(signature), candidates, self.inverted_index, self.related_thresh, match_map ) return self.verifier.get_related_sets(r_tokens, candidates, self.inverted_index), candidates_start , len(candidates)","title":"search_sets"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.set_alpha","text":"Updates the similarity threshold. Parameters: Name Type Description Default sim_thresh float Similarity threshold alpha required Source code in src/silkmoth/silkmoth_engine.py def set_alpha(self, sim_thresh): \"\"\" Updates the similarity threshold. Args: sim_thresh (float): Similarity threshold alpha \"\"\" self.sim_thresh = sim_thresh self.verifier = self._create_verifier() self.candidate_selector = self._create_candidate_selector()","title":"set_alpha"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.set_check_filter","text":"Updates the check filter flag. Parameters: Name Type Description Default is_check_filter bool Flag to activate/deactivate check filter required Source code in src/silkmoth/silkmoth_engine.py def set_check_filter(self, is_check_filter): \"\"\" Updates the check filter flag. Args: is_check_filter (bool): Flag to activate/deactivate check filter \"\"\" self.is_check_filter = is_check_filter","title":"set_check_filter"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.set_nn_filter","text":"Updates the nearest neighbor filter flag. Parameters: Name Type Description Default is_nn_filter bool Flag to activate/deactivate nearest neighbor filter required Source code in src/silkmoth/silkmoth_engine.py def set_nn_filter(self, is_nn_filter): \"\"\" Updates the nearest neighbor filter flag. Args: is_nn_filter (bool): Flag to activate/deactivate nearest neighbor filter \"\"\" self.is_nn_filter = is_nn_filter","title":"set_nn_filter"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.set_q","text":"Updates q-gram size. Parameters: Name Type Description Default q int The q-gram size for tokenization required Source code in src/silkmoth/silkmoth_engine.py def set_q(self, q): \"\"\" Updates q-gram size. Args: q (int): The q-gram size for tokenization \"\"\" self.q = q self.tokenizer = Tokenizer(self.sim_func, q) self.inverted_index = self.build_index(self.source_sets) self.signature_gen = SignatureGenerator() self.candidate_selector = self._create_candidate_selector() self.verifier = self._create_verifier()","title":"set_q"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.set_reduction","text":"Updates the reduction flag. Parameters: Name Type Description Default reduction bool Flag to activate/deactivate reduction required Source code in src/silkmoth/silkmoth_engine.py def set_reduction(self, reduction): \"\"\" Updates the reduction flag. Args: reduction (bool): Flag to activate/deactivate reduction \"\"\" self.reduction = reduction self.verifier = self._create_verifier()","title":"set_reduction"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.set_related_threshold","text":"Updates the relatedness threshold. Parameters: Name Type Description Default related_thresh float Relatedness threshold delta required Source code in src/silkmoth/silkmoth_engine.py def set_related_threshold(self, related_thresh): \"\"\" Updates the relatedness threshold. Args: related_thresh (float): Relatedness threshold delta \"\"\" self.related_thresh = related_thresh self.verifier = self._create_verifier() self.candidate_selector = self._create_candidate_selector()","title":"set_related_threshold"},{"location":"pages/silkmoth_engine/#silkmoth.silkmoth_engine.SilkMothEngine.set_signature_type","text":"Updates the signature type. Parameters: Name Type Description Default sig_type SigType Signature type required Source code in src/silkmoth/silkmoth_engine.py def set_signature_type(self, sig_type): \"\"\" Updates the signature type. Args: sig_type (SigType): Signature type \"\"\" self.signature_type = sig_type","title":"set_signature_type"},{"location":"pages/tokenizer/","text":"Tokenizer Source code in src/silkmoth/tokenizer.py class Tokenizer: def __init__(self, sim_func, q=3): \"\"\" Initialize the Tokenizer with a similarity function. Args: sim_func (callable): The similarity function that influences tokenization behavior. q (int): The q-gram size for tokenization, default is 3. \"\"\" self.sim_func = sim_func self.q = q def tokenize(self, input_set: list) -> list: \"\"\" Tokenizes the input based on the similarity function. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input. \"\"\" if self.sim_func == jaccard_similarity: tokens = jaccard_tokenize(input_set) elif self.sim_func == edit_similarity or self.sim_func == N_edit_similarity: tokens = qgram_tokenize(input_set, self.q) else: raise ValueError(\"Unsupported similarity function\") return tokens __init__(sim_func, q=3) Initialize the Tokenizer with a similarity function. Parameters: Name Type Description Default sim_func callable The similarity function that influences tokenization behavior. required q int The q-gram size for tokenization, default is 3. 3 Source code in src/silkmoth/tokenizer.py def __init__(self, sim_func, q=3): \"\"\" Initialize the Tokenizer with a similarity function. Args: sim_func (callable): The similarity function that influences tokenization behavior. q (int): The q-gram size for tokenization, default is 3. \"\"\" self.sim_func = sim_func self.q = q tokenize(input_set) Tokenizes the input based on the similarity function. Parameters: Name Type Description Default input_set list The input set to tokenize. required Returns: Name Type Description list list A list of str tokens extracted from the input. Source code in src/silkmoth/tokenizer.py def tokenize(self, input_set: list) -> list: \"\"\" Tokenizes the input based on the similarity function. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input. \"\"\" if self.sim_func == jaccard_similarity: tokens = jaccard_tokenize(input_set) elif self.sim_func == edit_similarity or self.sim_func == N_edit_similarity: tokens = qgram_tokenize(input_set, self.q) else: raise ValueError(\"Unsupported similarity function\") return tokens jaccard_tokenize(input_set) Tokenizes the input using Jaccard similarity. Parameters: Name Type Description Default input_set list The input set to tokenize. required Returns: Name Type Description list list A list of str tokens extracted from the input string. Source code in src/silkmoth/tokenizer.py def jaccard_tokenize(input_set: list) -> list: \"\"\" Tokenizes the input using Jaccard similarity. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input string. \"\"\" tokens = [] for element in input_set: if isinstance(element, (str, int, float, bool)): tokens.append(set(str(element).split())) elif isinstance(element, (list, tuple)): sub_tokens = set() for sub_element in element: if isinstance(sub_element, (str, int, float, bool)): sub_tokens.update(str(sub_element).split()) elif isinstance(sub_element, (list, tuple)): for sub_sub_element in sub_element: if isinstance(sub_sub_element, (str, int, float, bool)): sub_tokens.update(str(sub_sub_element).split()) else: raise ValueError( f\"Unsupported nested type: {type(sub_element)}\" ) else: raise ValueError( f\"Unsupported nested type: {type(sub_element)}\" ) tokens.append(sub_tokens) else: raise ValueError(f\"Unsupported element type: {type(element)}\") return tokens qgram_tokenize(input_set, q) Tokenizes the input using q-gram tokenization. Parameters: Name Type Description Default input_set list Input set with strings or nested values. required q int Length of q-gram. required Returns: Type Description list [ list [ str ]] list[list[str]]: A list of lists, each containing ordered q-gram tokens. Source code in src/silkmoth/tokenizer.py def qgram_tokenize(input_set: list, q: int) -> list[list[str]]: \"\"\" Tokenizes the input using q-gram tokenization. Args: input_set (list): Input set with strings or nested values. q (int): Length of q-gram. Returns: list[list[str]]: A list of lists, each containing ordered q-gram tokens. \"\"\" def to_qgrams(s: str) -> list[str]: s = s.strip() if len(s) < q: return [] return [s[i:i+q] for i in range(len(s) - q + 1)] def flatten(x): for el in x: if isinstance(el, (list, tuple)): yield from flatten(el) else: yield el tokens = [] for element in input_set: if isinstance(element, (str, int, float, bool)): s = str(element) elif isinstance(element, (list, tuple)): # Flatten nested elements and join with space s = \" \".join(str(x) for x in flatten(element)) else: raise ValueError(f\"Unsupported element type: {type(element)}\") tokens.append(to_qgrams(s)) # generate q-grams for the full string return tokens","title":"Tokenizer"},{"location":"pages/tokenizer/#silkmoth.tokenizer.Tokenizer","text":"Source code in src/silkmoth/tokenizer.py class Tokenizer: def __init__(self, sim_func, q=3): \"\"\" Initialize the Tokenizer with a similarity function. Args: sim_func (callable): The similarity function that influences tokenization behavior. q (int): The q-gram size for tokenization, default is 3. \"\"\" self.sim_func = sim_func self.q = q def tokenize(self, input_set: list) -> list: \"\"\" Tokenizes the input based on the similarity function. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input. \"\"\" if self.sim_func == jaccard_similarity: tokens = jaccard_tokenize(input_set) elif self.sim_func == edit_similarity or self.sim_func == N_edit_similarity: tokens = qgram_tokenize(input_set, self.q) else: raise ValueError(\"Unsupported similarity function\") return tokens","title":"Tokenizer"},{"location":"pages/tokenizer/#silkmoth.tokenizer.Tokenizer.__init__","text":"Initialize the Tokenizer with a similarity function. Parameters: Name Type Description Default sim_func callable The similarity function that influences tokenization behavior. required q int The q-gram size for tokenization, default is 3. 3 Source code in src/silkmoth/tokenizer.py def __init__(self, sim_func, q=3): \"\"\" Initialize the Tokenizer with a similarity function. Args: sim_func (callable): The similarity function that influences tokenization behavior. q (int): The q-gram size for tokenization, default is 3. \"\"\" self.sim_func = sim_func self.q = q","title":"__init__"},{"location":"pages/tokenizer/#silkmoth.tokenizer.Tokenizer.tokenize","text":"Tokenizes the input based on the similarity function. Parameters: Name Type Description Default input_set list The input set to tokenize. required Returns: Name Type Description list list A list of str tokens extracted from the input. Source code in src/silkmoth/tokenizer.py def tokenize(self, input_set: list) -> list: \"\"\" Tokenizes the input based on the similarity function. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input. \"\"\" if self.sim_func == jaccard_similarity: tokens = jaccard_tokenize(input_set) elif self.sim_func == edit_similarity or self.sim_func == N_edit_similarity: tokens = qgram_tokenize(input_set, self.q) else: raise ValueError(\"Unsupported similarity function\") return tokens","title":"tokenize"},{"location":"pages/tokenizer/#silkmoth.tokenizer.jaccard_tokenize","text":"Tokenizes the input using Jaccard similarity. Parameters: Name Type Description Default input_set list The input set to tokenize. required Returns: Name Type Description list list A list of str tokens extracted from the input string. Source code in src/silkmoth/tokenizer.py def jaccard_tokenize(input_set: list) -> list: \"\"\" Tokenizes the input using Jaccard similarity. Args: input_set: The input set to tokenize. Returns: list: A list of str tokens extracted from the input string. \"\"\" tokens = [] for element in input_set: if isinstance(element, (str, int, float, bool)): tokens.append(set(str(element).split())) elif isinstance(element, (list, tuple)): sub_tokens = set() for sub_element in element: if isinstance(sub_element, (str, int, float, bool)): sub_tokens.update(str(sub_element).split()) elif isinstance(sub_element, (list, tuple)): for sub_sub_element in sub_element: if isinstance(sub_sub_element, (str, int, float, bool)): sub_tokens.update(str(sub_sub_element).split()) else: raise ValueError( f\"Unsupported nested type: {type(sub_element)}\" ) else: raise ValueError( f\"Unsupported nested type: {type(sub_element)}\" ) tokens.append(sub_tokens) else: raise ValueError(f\"Unsupported element type: {type(element)}\") return tokens","title":"jaccard_tokenize"},{"location":"pages/tokenizer/#silkmoth.tokenizer.qgram_tokenize","text":"Tokenizes the input using q-gram tokenization. Parameters: Name Type Description Default input_set list Input set with strings or nested values. required q int Length of q-gram. required Returns: Type Description list [ list [ str ]] list[list[str]]: A list of lists, each containing ordered q-gram tokens. Source code in src/silkmoth/tokenizer.py def qgram_tokenize(input_set: list, q: int) -> list[list[str]]: \"\"\" Tokenizes the input using q-gram tokenization. Args: input_set (list): Input set with strings or nested values. q (int): Length of q-gram. Returns: list[list[str]]: A list of lists, each containing ordered q-gram tokens. \"\"\" def to_qgrams(s: str) -> list[str]: s = s.strip() if len(s) < q: return [] return [s[i:i+q] for i in range(len(s) - q + 1)] def flatten(x): for el in x: if isinstance(el, (list, tuple)): yield from flatten(el) else: yield el tokens = [] for element in input_set: if isinstance(element, (str, int, float, bool)): s = str(element) elif isinstance(element, (list, tuple)): # Flatten nested elements and join with space s = \" \".join(str(x) for x in flatten(element)) else: raise ValueError(f\"Unsupported element type: {type(element)}\") tokens.append(to_qgrams(s)) # generate q-grams for the full string return tokens","title":"qgram_tokenize"},{"location":"pages/utils/","text":"SigType Bases: Enum Signature type enum. Source code in src/silkmoth/utils.py class SigType(Enum): \"\"\" Signature type enum. \"\"\" WEIGHTED = \"weighted\" SKYLINE = \"skyline\" DICHOTOMY = \"dichotomy\" N_edit_similarity(x, y, sim_thresh=0) Computes the normalized edit similarity NEds between two strings or sets/lists of tokens: \\(NEds(x, y) = 1 - LD(x, y) / max(|x|, |y|)\\) Parameters: Name Type Description Default x str or set/list of str First input required y str or set/list of str Second input required sim_thresh float Similarity threshold (default: 0) 0 Returns: Name Type Description float float Similarity score in [0, 1], or 0 if below threshold Source code in src/silkmoth/utils.py def N_edit_similarity(x, y, sim_thresh=0) -> float: \"\"\" Computes the normalized edit similarity NEds between two strings or sets/lists of tokens: $NEds(x, y) = 1 - LD(x, y) / max(|x|, |y|)$ Args: x (str or set/list of str): First input y (str or set/list of str): Second input sim_thresh (float): Similarity threshold (default: 0) Returns: float: Similarity score in [0, 1], or 0 if below threshold \"\"\" x_str = reverse_qgrams(x) y_str = reverse_qgrams(y) if not x_str or not y_str: return .0 ld = Levenshtein.distance(x_str, y_str) max_len = max(len(x_str), len(y_str)) if max_len == 0: return 1.0 neds_score = 1 - (ld / max_len) return neds_score if neds_score >= sim_thresh else .0 contain(reference_set_size, source_set_size, mm_score) Computes Set-Containment metric which checks whether one set S is approximately a superset of another set R. Set pairs (R, S) with \\(|R| > |S|\\) should be filtered in advance. Set-Containment is defined as \\(contain(R, S) = mm\\_score / |R|\\) . Examples >>> from silkmoth.utils contain >>> contain(2, 3, 2) 1.0 >>> contain(2, 3, 1.5) 0.75 Parameters: Name Type Description Default reference_set_size int Size of set R required source_set_size int Size of set S required mm_score float Maximum matching score of R and S required Returns: Name Type Description float float Set-Containment Source code in src/silkmoth/utils.py def contain(reference_set_size: int, source_set_size: int, mm_score: float) -> float: \"\"\" Computes Set-Containment metric which checks whether one set S is approximately a superset of another set R. Set pairs (R, S) with $|R| > |S|$ should be filtered in advance. Set-Containment is defined as $contain(R, S) = mm\\_score / |R|$. Examples -------- ``` >>> from silkmoth.utils contain >>> contain(2, 3, 2) 1.0 >>> contain(2, 3, 1.5) 0.75 ``` Args: reference_set_size: Size of set R source_set_size: Size of set S mm_score: Maximum matching score of R and S Returns: float: Set-Containment \"\"\" if reference_set_size > source_set_size: raise ValueError(f\"Reference set too large\") return mm_score / reference_set_size edit_similarity(x, y, sim_thresh=0) Computes the edit similarity between two strings based on the formula given in the SILKMOTH paper: \\(Eds(x, y) = 1 - (2 * LD(x, y)) / (|x| + |y| + LD(x, y))\\) Parameters: Name Type Description Default x str or set/list of str First input required y str or set/list of str Second input required sim_thresh float Similarity threshold alpha (default is 0) 0 Returns: Name Type Description float float Edit similarity score (0 if below threshold) Source code in src/silkmoth/utils.py def edit_similarity (x, y, sim_thresh=0) -> float: \"\"\" Computes the edit similarity between two strings based on the formula given in the SILKMOTH paper: $Eds(x, y) = 1 - (2 * LD(x, y)) / (|x| + |y| + LD(x, y))$ Args: x (str or set/list of str): First input y (str or set/list of str): Second input sim_thresh (float): Similarity threshold alpha (default is 0) Returns: float: Edit similarity score (0 if below threshold) \"\"\" x_str = reverse_qgrams(x) y_str = reverse_qgrams(y) if not x_str or not y_str: return .0 ld = Levenshtein.distance(x_str, y_str) eds = 1 - (2 * ld) / (len(x_str) + len(y_str) + ld) return eds if eds >= sim_thresh else .0 flatten_tokens(input_val) Flattens a set, list of sets, or other nested iterable into a flat list of strings. Source code in src/silkmoth/utils.py def flatten_tokens(input_val): \"\"\" Flattens a set, list of sets, or other nested iterable into a flat list of strings. \"\"\" if isinstance(input_val, (set, list)): flat = [] for elem in input_val: if isinstance(elem, (set, list)): flat.extend(elem) else: flat.append(elem) return \" \".join(flat) return input_val # assume it's already a string jaccard_similarity(x, y, sim_thresh=0) Gives the Jaccard similarity of two set-like objects. Jaccard similarity is defined as \\(Jac(x, y) = |x \\cap y|/|x \\cup y|\\) . For some applications we may want to omit pairs with low similarity. Therefore a similarity threshold \u03b1 is provided. If the similarity score does not exceed this threshold, this function returns zero. Examples >>> from silkmoth.utils import jaccard_similarity >>> x = {\"a\", \"b\", \"c\"} >>> y = {\"a\", \"b\", \"c\"} >>> jaccard_similarity(x, y) 1.0 >>> y.add(\"d\") >>> jaccard_similarity(x, y) 0.75 >>> jaccard_similarity(x, y, 0.8) 0.0 Parameters: Name Type Description Default x set Input element x required y set Input element y required sim_thresh float Similarity threshold alpha 0 Returns: Name Type Description float float Jaccard similarity score Source code in src/silkmoth/utils.py def jaccard_similarity(x: set, y: set, sim_thresh=0) -> float: \"\"\" Gives the Jaccard similarity of two set-like objects. Jaccard similarity is defined as $Jac(x, y) = |x \\cap y|/|x \\cup y|$. For some applications we may want to omit pairs with low similarity. Therefore a similarity threshold \u03b1 is provided. If the similarity score does not exceed this threshold, this function returns zero. Examples -------- ``` >>> from silkmoth.utils import jaccard_similarity >>> x = {\"a\", \"b\", \"c\"} >>> y = {\"a\", \"b\", \"c\"} >>> jaccard_similarity(x, y) 1.0 >>> y.add(\"d\") >>> jaccard_similarity(x, y) 0.75 >>> jaccard_similarity(x, y, 0.8) 0.0 ``` Args: x (set): Input element x y (set): Input element y sim_thresh (float): Similarity threshold alpha Returns: float: Jaccard similarity score \"\"\" if len(x) == 0 or len(y) == 0: return .0 jac = len(x & y) / len(x | y) if jac >= sim_thresh: return jac return .0 reverse_qgrams(input_val) Reverse qgrams back to their original text. Source code in src/silkmoth/utils.py def reverse_qgrams(input_val) -> str: \"\"\" Reverse qgrams back to their original text. \"\"\" if isinstance(input_val, (list,OrderedSet)): if len(input_val) == 0: return \"\" if len(input_val) == 1: return input_val[0] result = \"\" for gram in input_val[:-1]: result += gram[0] last_gram = input_val[-1] return result + last_gram return input_val # assume it's already a string similar(reference_set_size, source_set_size, mm_score) Computes Set-Similarity metric which checks whether two sets R and S are approximately equivalent. Set-Similarity is defined as \\(similar(R, S) = mm\\_score / (|R| + |S| - mm\\_score)\\) . Examples >>> from silkmoth.utils import similar >>> similar(3, 3, 3) 1.0 >>> similar(3, 3, 1.5) 0.3333333333333333 Parameters: Name Type Description Default reference_set_size int Size of set R required source_set_size int Size of set S required mm_score float Maximum matching score of R and S required Returns: Name Type Description float float Set-Similarity Source code in src/silkmoth/utils.py def similar(reference_set_size: int, source_set_size: int, mm_score: float) -> float: \"\"\" Computes Set-Similarity metric which checks whether two sets R and S are approximately equivalent. Set-Similarity is defined as $similar(R, S) = mm\\_score / (|R| + |S| - mm\\_score)$. Examples -------- ``` >>> from silkmoth.utils import similar >>> similar(3, 3, 3) 1.0 >>> similar(3, 3, 1.5) 0.3333333333333333 ``` Args: reference_set_size: Size of set R source_set_size: Size of set S mm_score: Maximum matching score of R and S Returns: float: Set-Similarity \"\"\" return mm_score / (reference_set_size + source_set_size - mm_score)","title":"Utils"},{"location":"pages/utils/#silkmoth.utils.SigType","text":"Bases: Enum Signature type enum. Source code in src/silkmoth/utils.py class SigType(Enum): \"\"\" Signature type enum. \"\"\" WEIGHTED = \"weighted\" SKYLINE = \"skyline\" DICHOTOMY = \"dichotomy\"","title":"SigType"},{"location":"pages/utils/#silkmoth.utils.N_edit_similarity","text":"Computes the normalized edit similarity NEds between two strings or sets/lists of tokens: \\(NEds(x, y) = 1 - LD(x, y) / max(|x|, |y|)\\) Parameters: Name Type Description Default x str or set/list of str First input required y str or set/list of str Second input required sim_thresh float Similarity threshold (default: 0) 0 Returns: Name Type Description float float Similarity score in [0, 1], or 0 if below threshold Source code in src/silkmoth/utils.py def N_edit_similarity(x, y, sim_thresh=0) -> float: \"\"\" Computes the normalized edit similarity NEds between two strings or sets/lists of tokens: $NEds(x, y) = 1 - LD(x, y) / max(|x|, |y|)$ Args: x (str or set/list of str): First input y (str or set/list of str): Second input sim_thresh (float): Similarity threshold (default: 0) Returns: float: Similarity score in [0, 1], or 0 if below threshold \"\"\" x_str = reverse_qgrams(x) y_str = reverse_qgrams(y) if not x_str or not y_str: return .0 ld = Levenshtein.distance(x_str, y_str) max_len = max(len(x_str), len(y_str)) if max_len == 0: return 1.0 neds_score = 1 - (ld / max_len) return neds_score if neds_score >= sim_thresh else .0","title":"N_edit_similarity"},{"location":"pages/utils/#silkmoth.utils.contain","text":"Computes Set-Containment metric which checks whether one set S is approximately a superset of another set R. Set pairs (R, S) with \\(|R| > |S|\\) should be filtered in advance. Set-Containment is defined as \\(contain(R, S) = mm\\_score / |R|\\) .","title":"contain"},{"location":"pages/utils/#silkmoth.utils.contain--examples","text":">>> from silkmoth.utils contain >>> contain(2, 3, 2) 1.0 >>> contain(2, 3, 1.5) 0.75 Parameters: Name Type Description Default reference_set_size int Size of set R required source_set_size int Size of set S required mm_score float Maximum matching score of R and S required Returns: Name Type Description float float Set-Containment Source code in src/silkmoth/utils.py def contain(reference_set_size: int, source_set_size: int, mm_score: float) -> float: \"\"\" Computes Set-Containment metric which checks whether one set S is approximately a superset of another set R. Set pairs (R, S) with $|R| > |S|$ should be filtered in advance. Set-Containment is defined as $contain(R, S) = mm\\_score / |R|$. Examples -------- ``` >>> from silkmoth.utils contain >>> contain(2, 3, 2) 1.0 >>> contain(2, 3, 1.5) 0.75 ``` Args: reference_set_size: Size of set R source_set_size: Size of set S mm_score: Maximum matching score of R and S Returns: float: Set-Containment \"\"\" if reference_set_size > source_set_size: raise ValueError(f\"Reference set too large\") return mm_score / reference_set_size","title":"Examples"},{"location":"pages/utils/#silkmoth.utils.edit_similarity","text":"Computes the edit similarity between two strings based on the formula given in the SILKMOTH paper: \\(Eds(x, y) = 1 - (2 * LD(x, y)) / (|x| + |y| + LD(x, y))\\) Parameters: Name Type Description Default x str or set/list of str First input required y str or set/list of str Second input required sim_thresh float Similarity threshold alpha (default is 0) 0 Returns: Name Type Description float float Edit similarity score (0 if below threshold) Source code in src/silkmoth/utils.py def edit_similarity (x, y, sim_thresh=0) -> float: \"\"\" Computes the edit similarity between two strings based on the formula given in the SILKMOTH paper: $Eds(x, y) = 1 - (2 * LD(x, y)) / (|x| + |y| + LD(x, y))$ Args: x (str or set/list of str): First input y (str or set/list of str): Second input sim_thresh (float): Similarity threshold alpha (default is 0) Returns: float: Edit similarity score (0 if below threshold) \"\"\" x_str = reverse_qgrams(x) y_str = reverse_qgrams(y) if not x_str or not y_str: return .0 ld = Levenshtein.distance(x_str, y_str) eds = 1 - (2 * ld) / (len(x_str) + len(y_str) + ld) return eds if eds >= sim_thresh else .0","title":"edit_similarity"},{"location":"pages/utils/#silkmoth.utils.flatten_tokens","text":"Flattens a set, list of sets, or other nested iterable into a flat list of strings. Source code in src/silkmoth/utils.py def flatten_tokens(input_val): \"\"\" Flattens a set, list of sets, or other nested iterable into a flat list of strings. \"\"\" if isinstance(input_val, (set, list)): flat = [] for elem in input_val: if isinstance(elem, (set, list)): flat.extend(elem) else: flat.append(elem) return \" \".join(flat) return input_val # assume it's already a string","title":"flatten_tokens"},{"location":"pages/utils/#silkmoth.utils.jaccard_similarity","text":"Gives the Jaccard similarity of two set-like objects. Jaccard similarity is defined as \\(Jac(x, y) = |x \\cap y|/|x \\cup y|\\) . For some applications we may want to omit pairs with low similarity. Therefore a similarity threshold \u03b1 is provided. If the similarity score does not exceed this threshold, this function returns zero.","title":"jaccard_similarity"},{"location":"pages/utils/#silkmoth.utils.jaccard_similarity--examples","text":">>> from silkmoth.utils import jaccard_similarity >>> x = {\"a\", \"b\", \"c\"} >>> y = {\"a\", \"b\", \"c\"} >>> jaccard_similarity(x, y) 1.0 >>> y.add(\"d\") >>> jaccard_similarity(x, y) 0.75 >>> jaccard_similarity(x, y, 0.8) 0.0 Parameters: Name Type Description Default x set Input element x required y set Input element y required sim_thresh float Similarity threshold alpha 0 Returns: Name Type Description float float Jaccard similarity score Source code in src/silkmoth/utils.py def jaccard_similarity(x: set, y: set, sim_thresh=0) -> float: \"\"\" Gives the Jaccard similarity of two set-like objects. Jaccard similarity is defined as $Jac(x, y) = |x \\cap y|/|x \\cup y|$. For some applications we may want to omit pairs with low similarity. Therefore a similarity threshold \u03b1 is provided. If the similarity score does not exceed this threshold, this function returns zero. Examples -------- ``` >>> from silkmoth.utils import jaccard_similarity >>> x = {\"a\", \"b\", \"c\"} >>> y = {\"a\", \"b\", \"c\"} >>> jaccard_similarity(x, y) 1.0 >>> y.add(\"d\") >>> jaccard_similarity(x, y) 0.75 >>> jaccard_similarity(x, y, 0.8) 0.0 ``` Args: x (set): Input element x y (set): Input element y sim_thresh (float): Similarity threshold alpha Returns: float: Jaccard similarity score \"\"\" if len(x) == 0 or len(y) == 0: return .0 jac = len(x & y) / len(x | y) if jac >= sim_thresh: return jac return .0","title":"Examples"},{"location":"pages/utils/#silkmoth.utils.reverse_qgrams","text":"Reverse qgrams back to their original text. Source code in src/silkmoth/utils.py def reverse_qgrams(input_val) -> str: \"\"\" Reverse qgrams back to their original text. \"\"\" if isinstance(input_val, (list,OrderedSet)): if len(input_val) == 0: return \"\" if len(input_val) == 1: return input_val[0] result = \"\" for gram in input_val[:-1]: result += gram[0] last_gram = input_val[-1] return result + last_gram return input_val # assume it's already a string","title":"reverse_qgrams"},{"location":"pages/utils/#silkmoth.utils.similar","text":"Computes Set-Similarity metric which checks whether two sets R and S are approximately equivalent. Set-Similarity is defined as \\(similar(R, S) = mm\\_score / (|R| + |S| - mm\\_score)\\) .","title":"similar"},{"location":"pages/utils/#silkmoth.utils.similar--examples","text":">>> from silkmoth.utils import similar >>> similar(3, 3, 3) 1.0 >>> similar(3, 3, 1.5) 0.3333333333333333 Parameters: Name Type Description Default reference_set_size int Size of set R required source_set_size int Size of set S required mm_score float Maximum matching score of R and S required Returns: Name Type Description float float Set-Similarity Source code in src/silkmoth/utils.py def similar(reference_set_size: int, source_set_size: int, mm_score: float) -> float: \"\"\" Computes Set-Similarity metric which checks whether two sets R and S are approximately equivalent. Set-Similarity is defined as $similar(R, S) = mm\\_score / (|R| + |S| - mm\\_score)$. Examples -------- ``` >>> from silkmoth.utils import similar >>> similar(3, 3, 3) 1.0 >>> similar(3, 3, 1.5) 0.3333333333333333 ``` Args: reference_set_size: Size of set R source_set_size: Size of set S mm_score: Maximum matching score of R and S Returns: float: Set-Similarity \"\"\" return mm_score / (reference_set_size + source_set_size - mm_score)","title":"Examples"},{"location":"pages/verifier/","text":"Verifier The verifier component executes the final verification step in the SilkMoth pipeline. During verification SilkMoth performs the maximum matching between every candidate set and the reference set R. The sets whose maximum matching score surpass the relatedness threshold \u03b4 are the verified related sets to R. For maximum matching computation we treat every element of the two sets as vertices of a bipartite graph and the weights of each edge determined by the similarity function. The maximum weighted matching is computed using the existing library SciPy . Optionally, a triangle inequality-based reduction can be applied to further improve performance. Examples >>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.verifier import Verifier >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Berlin\", \"Sun\"}] >>> verifier = Verifier(0.1, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(0, 0.17073170731707313), (1, 0.7142857142857142)] >>> verifier = Verifier(0.7, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(1, 0.7142857142857142)] Source code in src/silkmoth/verifier.py class Verifier: \"\"\" The verifier component executes the final verification step in the SilkMoth pipeline. During verification SilkMoth performs the maximum matching between every candidate set and the reference set R. The sets whose maximum matching score surpass the relatedness threshold \u03b4 are the verified related sets to R. For maximum matching computation we treat every element of the two sets as vertices of a bipartite graph and the weights of each edge determined by the similarity function. The maximum weighted matching is computed using the existing library [SciPy](https://scipy.org/). Optionally, a triangle inequality-based reduction can be applied to further improve performance. Examples -------- ``` >>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.verifier import Verifier >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Berlin\", \"Sun\"}] >>> verifier = Verifier(0.1, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(0, 0.17073170731707313), (1, 0.7142857142857142)] >>> verifier = Verifier(0.7, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(1, 0.7142857142857142)] ``` \"\"\" def __init__(self, related_thresh, sim_metric, sim_func, sim_thresh=0, reduction=False): \"\"\" Initialize the verifier with some parameters. Args: related_thresh (float): Relatedness threshold delta sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction \"\"\" self.related_thresh = related_thresh self.sim_metric = sim_metric self.sim_func = sim_func self.sim_thresh = sim_thresh self.reduction = reduction def get_mm_score(self, reference_set, source_set) -> float: \"\"\" Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Maximum matching score (sum of weights of edges in the matching) \"\"\" n, m = len(reference_set), len(source_set) if n == 0 or m == 0: return 0.0 weights = np.zeros((n, m), dtype=float) for i, r_elem in enumerate(reference_set): for j, s_elem in enumerate(source_set): weights[i, j] = self.sim_func(r_elem, s_elem, self.sim_thresh) # use negative weights to search for minimal cost cost = -weights row_ind, col_ind = linear_sum_assignment(cost) return float(weights[row_ind, col_ind].sum()) def get_relatedness(self, reference_set, source_set) -> float: \"\"\" Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Relatedness score of R and S \"\"\" r_size = len(reference_set) s_size = len(source_set) exact_matches = 0 if self.reduction: reference_set, source_set, exact_matches = reduce_sets(reference_set, source_set) mm_score = self.get_mm_score(reference_set, source_set) + exact_matches relatedness = self.sim_metric(r_size, s_size, mm_score) return relatedness def get_related_sets(self, reference_set: list, candidates: set, inverted_index: InvertedIndex) -> list: \"\"\" Gives all candidate sets that are related to the reference set. Args: reference_set (list): Tokeinized reference set R candidates (set): Collection of indices of candidate sets inverted_index (InvertedIndex): Inverted index instance Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. \"\"\" related_sets = [] for c in candidates: source_set = inverted_index.get_set(c) relatedness = self.get_relatedness(reference_set, source_set) if relatedness >= self.related_thresh: related_sets.append((c, relatedness)) return related_sets __init__(related_thresh, sim_metric, sim_func, sim_thresh=0, reduction=False) Initialize the verifier with some parameters. Parameters: Name Type Description Default related_thresh float Relatedness threshold delta required sim_metric callable Similarity metric similar(...)/contain(...) required sim_func callable Similarity function phi required sim_thresh float Similarity threshold alpha 0 reduction bool Flag to activate/deactivate triangle inequality reduction False Source code in src/silkmoth/verifier.py def __init__(self, related_thresh, sim_metric, sim_func, sim_thresh=0, reduction=False): \"\"\" Initialize the verifier with some parameters. Args: related_thresh (float): Relatedness threshold delta sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction \"\"\" self.related_thresh = related_thresh self.sim_metric = sim_metric self.sim_func = sim_func self.sim_thresh = sim_thresh self.reduction = reduction get_mm_score(reference_set, source_set) Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Name Type Description float float Maximum matching score (sum of weights of edges in the matching) Source code in src/silkmoth/verifier.py def get_mm_score(self, reference_set, source_set) -> float: \"\"\" Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Maximum matching score (sum of weights of edges in the matching) \"\"\" n, m = len(reference_set), len(source_set) if n == 0 or m == 0: return 0.0 weights = np.zeros((n, m), dtype=float) for i, r_elem in enumerate(reference_set): for j, s_elem in enumerate(source_set): weights[i, j] = self.sim_func(r_elem, s_elem, self.sim_thresh) # use negative weights to search for minimal cost cost = -weights row_ind, col_ind = linear_sum_assignment(cost) return float(weights[row_ind, col_ind].sum()) get_related_sets(reference_set, candidates, inverted_index) Gives all candidate sets that are related to the reference set. Parameters: Name Type Description Default reference_set list Tokeinized reference set R required candidates set Collection of indices of candidate sets required inverted_index InvertedIndex Inverted index instance required Returns: Name Type Description list list Pairs of indices of all related sets from the candidates and list their relatedness with the reference set. Source code in src/silkmoth/verifier.py def get_related_sets(self, reference_set: list, candidates: set, inverted_index: InvertedIndex) -> list: \"\"\" Gives all candidate sets that are related to the reference set. Args: reference_set (list): Tokeinized reference set R candidates (set): Collection of indices of candidate sets inverted_index (InvertedIndex): Inverted index instance Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. \"\"\" related_sets = [] for c in candidates: source_set = inverted_index.get_set(c) relatedness = self.get_relatedness(reference_set, source_set) if relatedness >= self.related_thresh: related_sets.append((c, relatedness)) return related_sets get_relatedness(reference_set, source_set) Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Name Type Description float float Relatedness score of R and S Source code in src/silkmoth/verifier.py def get_relatedness(self, reference_set, source_set) -> float: \"\"\" Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Relatedness score of R and S \"\"\" r_size = len(reference_set) s_size = len(source_set) exact_matches = 0 if self.reduction: reference_set, source_set, exact_matches = reduce_sets(reference_set, source_set) mm_score = self.get_mm_score(reference_set, source_set) + exact_matches relatedness = self.sim_metric(r_size, s_size, mm_score) return relatedness reduce_sets(reference_set, source_set) Applies the triangle inequality reduction by removing every element from both sets that has an identical match in the other set. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Type Description ( list , list , int ) Reduced reference set, reduced source set and number of identical elements. Source code in src/silkmoth/verifier.py def reduce_sets(reference_set: list, source_set: list) -> tuple: \"\"\" Applies the triangle inequality reduction by removing every element from both sets that has an identical match in the other set. Args: reference_set: Tokenized reference set R source_set: Tokenized source set S Returns: (list, list, int): Reduced reference set, reduced source set and number of identical elements. \"\"\" r_reduced = reference_set[:] s_reduced = source_set[:] count = 0 for elem in reference_set: if elem in s_reduced: s_reduced.remove(elem) r_reduced.remove(elem) count += 1 return (r_reduced, s_reduced, count)","title":"Verifier"},{"location":"pages/verifier/#silkmoth.verifier.Verifier","text":"The verifier component executes the final verification step in the SilkMoth pipeline. During verification SilkMoth performs the maximum matching between every candidate set and the reference set R. The sets whose maximum matching score surpass the relatedness threshold \u03b4 are the verified related sets to R. For maximum matching computation we treat every element of the two sets as vertices of a bipartite graph and the weights of each edge determined by the similarity function. The maximum weighted matching is computed using the existing library SciPy . Optionally, a triangle inequality-based reduction can be applied to further improve performance.","title":"Verifier"},{"location":"pages/verifier/#silkmoth.verifier.Verifier--examples","text":">>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.verifier import Verifier >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Berlin\", \"Sun\"}] >>> verifier = Verifier(0.1, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(0, 0.17073170731707313), (1, 0.7142857142857142)] >>> verifier = Verifier(0.7, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(1, 0.7142857142857142)] Source code in src/silkmoth/verifier.py class Verifier: \"\"\" The verifier component executes the final verification step in the SilkMoth pipeline. During verification SilkMoth performs the maximum matching between every candidate set and the reference set R. The sets whose maximum matching score surpass the relatedness threshold \u03b4 are the verified related sets to R. For maximum matching computation we treat every element of the two sets as vertices of a bipartite graph and the weights of each edge determined by the similarity function. The maximum weighted matching is computed using the existing library [SciPy](https://scipy.org/). Optionally, a triangle inequality-based reduction can be applied to further improve performance. Examples -------- ``` >>> from silkmoth.inverted_index import InvertedIndex >>> from silkmoth.utils import similar, jaccard_similarity >>> from silkmoth.verifier import Verifier >>> S1 = [{\"Apple\", \"Pear\", \"Car\"}, {\"Apple\", \"Sun\", \"Cat\"}] >>> S2 = [{\"Apple\", \"Berlin\", \"Sun\"}, {\"Apple\"}] >>> S = [S1, S2] >>> I = InvertedIndex(S) >>> R = [{\"Apple\"}, {\"Berlin\", \"Sun\"}] >>> verifier = Verifier(0.1, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(0, 0.17073170731707313), (1, 0.7142857142857142)] >>> verifier = Verifier(0.7, similar, jaccard_similarity) >>> verifier.get_related_sets(R, {0, 1}, I) [(1, 0.7142857142857142)] ``` \"\"\" def __init__(self, related_thresh, sim_metric, sim_func, sim_thresh=0, reduction=False): \"\"\" Initialize the verifier with some parameters. Args: related_thresh (float): Relatedness threshold delta sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction \"\"\" self.related_thresh = related_thresh self.sim_metric = sim_metric self.sim_func = sim_func self.sim_thresh = sim_thresh self.reduction = reduction def get_mm_score(self, reference_set, source_set) -> float: \"\"\" Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Maximum matching score (sum of weights of edges in the matching) \"\"\" n, m = len(reference_set), len(source_set) if n == 0 or m == 0: return 0.0 weights = np.zeros((n, m), dtype=float) for i, r_elem in enumerate(reference_set): for j, s_elem in enumerate(source_set): weights[i, j] = self.sim_func(r_elem, s_elem, self.sim_thresh) # use negative weights to search for minimal cost cost = -weights row_ind, col_ind = linear_sum_assignment(cost) return float(weights[row_ind, col_ind].sum()) def get_relatedness(self, reference_set, source_set) -> float: \"\"\" Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Relatedness score of R and S \"\"\" r_size = len(reference_set) s_size = len(source_set) exact_matches = 0 if self.reduction: reference_set, source_set, exact_matches = reduce_sets(reference_set, source_set) mm_score = self.get_mm_score(reference_set, source_set) + exact_matches relatedness = self.sim_metric(r_size, s_size, mm_score) return relatedness def get_related_sets(self, reference_set: list, candidates: set, inverted_index: InvertedIndex) -> list: \"\"\" Gives all candidate sets that are related to the reference set. Args: reference_set (list): Tokeinized reference set R candidates (set): Collection of indices of candidate sets inverted_index (InvertedIndex): Inverted index instance Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. \"\"\" related_sets = [] for c in candidates: source_set = inverted_index.get_set(c) relatedness = self.get_relatedness(reference_set, source_set) if relatedness >= self.related_thresh: related_sets.append((c, relatedness)) return related_sets","title":"Examples"},{"location":"pages/verifier/#silkmoth.verifier.Verifier.__init__","text":"Initialize the verifier with some parameters. Parameters: Name Type Description Default related_thresh float Relatedness threshold delta required sim_metric callable Similarity metric similar(...)/contain(...) required sim_func callable Similarity function phi required sim_thresh float Similarity threshold alpha 0 reduction bool Flag to activate/deactivate triangle inequality reduction False Source code in src/silkmoth/verifier.py def __init__(self, related_thresh, sim_metric, sim_func, sim_thresh=0, reduction=False): \"\"\" Initialize the verifier with some parameters. Args: related_thresh (float): Relatedness threshold delta sim_metric (callable): Similarity metric similar(...)/contain(...) sim_func (callable): Similarity function phi sim_thresh (float): Similarity threshold alpha reduction (bool): Flag to activate/deactivate triangle inequality reduction \"\"\" self.related_thresh = related_thresh self.sim_metric = sim_metric self.sim_func = sim_func self.sim_thresh = sim_thresh self.reduction = reduction","title":"__init__"},{"location":"pages/verifier/#silkmoth.verifier.Verifier.get_mm_score","text":"Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Name Type Description float float Maximum matching score (sum of weights of edges in the matching) Source code in src/silkmoth/verifier.py def get_mm_score(self, reference_set, source_set) -> float: \"\"\" Helper function that computes the maximum weighted bipartite matching score, where elements correspond to nodes and the edges are weighted using the similarity function. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Maximum matching score (sum of weights of edges in the matching) \"\"\" n, m = len(reference_set), len(source_set) if n == 0 or m == 0: return 0.0 weights = np.zeros((n, m), dtype=float) for i, r_elem in enumerate(reference_set): for j, s_elem in enumerate(source_set): weights[i, j] = self.sim_func(r_elem, s_elem, self.sim_thresh) # use negative weights to search for minimal cost cost = -weights row_ind, col_ind = linear_sum_assignment(cost) return float(weights[row_ind, col_ind].sum())","title":"get_mm_score"},{"location":"pages/verifier/#silkmoth.verifier.Verifier.get_related_sets","text":"Gives all candidate sets that are related to the reference set. Parameters: Name Type Description Default reference_set list Tokeinized reference set R required candidates set Collection of indices of candidate sets required inverted_index InvertedIndex Inverted index instance required Returns: Name Type Description list list Pairs of indices of all related sets from the candidates and list their relatedness with the reference set. Source code in src/silkmoth/verifier.py def get_related_sets(self, reference_set: list, candidates: set, inverted_index: InvertedIndex) -> list: \"\"\" Gives all candidate sets that are related to the reference set. Args: reference_set (list): Tokeinized reference set R candidates (set): Collection of indices of candidate sets inverted_index (InvertedIndex): Inverted index instance Returns: list: Pairs of indices of all related sets from the candidates and their relatedness with the reference set. \"\"\" related_sets = [] for c in candidates: source_set = inverted_index.get_set(c) relatedness = self.get_relatedness(reference_set, source_set) if relatedness >= self.related_thresh: related_sets.append((c, relatedness)) return related_sets","title":"get_related_sets"},{"location":"pages/verifier/#silkmoth.verifier.Verifier.get_relatedness","text":"Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Name Type Description float float Relatedness score of R and S Source code in src/silkmoth/verifier.py def get_relatedness(self, reference_set, source_set) -> float: \"\"\" Helper function that gives the relatedness score by computing the maximum weighted bipartite matching. Args: reference_set (list): Tokenized reference set R source_set (list): Tokenized source set S Returns: float: Relatedness score of R and S \"\"\" r_size = len(reference_set) s_size = len(source_set) exact_matches = 0 if self.reduction: reference_set, source_set, exact_matches = reduce_sets(reference_set, source_set) mm_score = self.get_mm_score(reference_set, source_set) + exact_matches relatedness = self.sim_metric(r_size, s_size, mm_score) return relatedness","title":"get_relatedness"},{"location":"pages/verifier/#silkmoth.verifier.reduce_sets","text":"Applies the triangle inequality reduction by removing every element from both sets that has an identical match in the other set. Parameters: Name Type Description Default reference_set list Tokenized reference set R required source_set list Tokenized source set S required Returns: Type Description ( list , list , int ) Reduced reference set, reduced source set and number of identical elements. Source code in src/silkmoth/verifier.py def reduce_sets(reference_set: list, source_set: list) -> tuple: \"\"\" Applies the triangle inequality reduction by removing every element from both sets that has an identical match in the other set. Args: reference_set: Tokenized reference set R source_set: Tokenized source set S Returns: (list, list, int): Reduced reference set, reduced source set and number of identical elements. \"\"\" r_reduced = reference_set[:] s_reduced = source_set[:] count = 0 for elem in reference_set: if elem in s_reduced: s_reduced.remove(elem) r_reduced.remove(elem) count += 1 return (r_reduced, s_reduced, count)","title":"reduce_sets"}]}