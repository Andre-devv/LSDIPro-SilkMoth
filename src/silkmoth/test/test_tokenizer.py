import unittest
from silkmoth.tokenizer import Tokenizer
from silkmoth.utils import jaccard_similarity


class TestTokenizer(unittest.TestCase):

    def setUp(self):
        self.tokenizer_jaccard = Tokenizer(sim_func=jaccard_similarity)
        self.tokenizer_unsupported = Tokenizer(sim_func=None)

    def test_unsupported_similarity_function(self):

        with self.assertRaises(ValueError):
            self.tokenizer_unsupported.tokenize("test string")

    def test_jaccard_tokenize_english(self):
        input_string = ["77 Mass Ave Boston MA"]
        expected_tokens = ["77", "Mass", "Ave", "Boston", "MA"]
        tokens = self.tokenizer_jaccard.tokenize(input_string)
        self.assertEqual(tokens, expected_tokens)

    def test_jaccard_tokenize_unicode(self):
        input_string = ["ã“ã‚“ã«ã¡ã¯ ä¸–ç•Œ ğŸŒ"]
        expected_tokens = ["ã“ã‚“ã«ã¡ã¯", "ä¸–ç•Œ", "ğŸŒ"]
        tokens = self.tokenizer_jaccard.tokenize(input_string)
        self.assertEqual(tokens, expected_tokens)

    def test_jaccard_tokenize_numbers(self):
        input_data = [123, 45.67, True]
        expected_tokens = ["123", "45.67", "True"]
        tokens = self.tokenizer_jaccard.tokenize(input_data)
        self.assertEqual(tokens, expected_tokens)

    def test_jaccard_tokenize_nested_lists(self):
        input_data = [["77 Mass Ave", "Boston"], ["MA", 123]]
        expected_tokens = ["77", "Mass", "Ave", "Boston", "MA", "123"]
        tokens = self.tokenizer_jaccard.tokenize(input_data)
        self.assertEqual(tokens, expected_tokens)

    def test_jaccard_tokenize_unsupported_type(self):
        input_data = [{"key": "value"}]  # Dictionaries are unsupported
        with self.assertRaises(ValueError):
            self.tokenizer_jaccard.tokenize(input_data)

    def test_jaccard_tokenize_unsupported_nested_type(self):
        input_data = [["77 Mass Ave", {"key": "value"}]]  # Nested dict is unsupported
        with self.assertRaises(ValueError):
            self.tokenizer_jaccard.tokenize(input_data)

    def test_jaccard_tokenize_empty_list(self):
        input_data = []
        expected_tokens = []
        tokens = self.tokenizer_jaccard.tokenize(input_data)
        self.assertEqual(tokens, expected_tokens)

    def test_jaccard_tokenize_mixed_types(self):
        input_data = ["Hello World", 123, [True, 45.67]]
        expected_tokens = ["Hello", "World", "123", "True", "45.67"]
        tokens = self.tokenizer_jaccard.tokenize(input_data)
        self.assertEqual(tokens, expected_tokens)


if __name__ == "__main__":
    unittest.main()
